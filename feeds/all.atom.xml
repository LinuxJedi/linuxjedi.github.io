<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>LinuxJedi's /dev/null</title><link href="http://linuxjedi.co.uk/" rel="alternate"></link><link href="http://linuxjedi.co.uk/feeds/all.atom.xml" rel="self"></link><id>http://linuxjedi.co.uk/</id><updated>2016-01-02T14:10:00+00:00</updated><entry><title>Multi-Gigabit Networking on a Budget</title><link href="http://linuxjedi.co.uk/posts/2016/Jan/02/multi-gigabit-networking-on-a-budget/" rel="alternate"></link><published>2016-01-02T14:10:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2016-01-02:posts/2016/Jan/02/multi-gigabit-networking-on-a-budget/</id><summary type="html">&lt;p&gt;I've recently acquired two &amp;quot;refurbished&amp;quot; Xeon workstations from eBay to do some &lt;a class="reference external" href="https://www.nginx.com/"&gt;NGINX&lt;/a&gt; testing with. These are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A Lenovo ThinkStation S30 - 6 core / 12 thread E5-1650v2 &amp;#64;3.5GHz, 4GB ECC RAM (upgraded to 12GB ECC - £10) - £650&lt;/li&gt;
&lt;li&gt;A Lenovo ThinkStation C20 - 2x 4 core / 8 thread (8 core / 16 thread total) E5620 &amp;#64;2.4GHz, 12GB ECC RAM - £200&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can maybe tell, I'm a fan of Lenovo hardware, I use a Thinkpad x220 still as my primary laptop despite also having newer laptops.&lt;/p&gt;
&lt;img alt="Lenovo ThinkStation S30 and C20" src="/images/xeon_workstations.jpg" style="width: 400px;" /&gt;
&lt;p&gt;The S30 was actually brand new, still sealed in box with Lenovo tape. Both machines were in great condition and run NGINX very well... Too well... The gigabit LAN in these was easily saturated without any real effort from the rest of the machines.&lt;/p&gt;
&lt;p&gt;I decided to look into ways of upgrading the network on these machines but without spending a fortune doing it.&lt;/p&gt;
&lt;div class="section" id="quad-port-bonded-gigabit"&gt;
&lt;h2&gt;Quad-port Bonded Gigabit&lt;/h2&gt;
&lt;p&gt;First up I looked into 4-port Gigabit network cards. For £40 each I got hold of an Intel Pro 1000 ET and an Intel Pro 1000 PT. The main difference between these cards appears to be the number of interrupt channels per port. The ET has 4 per port, the PT has 1. I have an HP ProCurve 16-port Gigabit switch I luckily picked up on eBay for £10 a while back so both cards were hooked into this using lots of short runs of CAT5e cabling.&lt;/p&gt;
&lt;p&gt;Intel Pro 1000 ET:&lt;/p&gt;
&lt;img alt="Intel Pro 1000 ET" src="/images/1000et.jpg" style="width: 400px;" /&gt;
&lt;p&gt;Intel Pro 1000 PT:&lt;/p&gt;
&lt;img alt="Intel Pro 1000 PT" src="/images/1000pt.jpg" style="width: 400px;" /&gt;
&lt;p&gt;I was using Fedora 23 in both machines, RedHat have an excellent guide on how to bond network connections using NetworkManager's CLI tool which can be seen &lt;a class="reference external" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Network_Bonding_Using_the_NetworkManager_Command_Line_Tool_nmcli.html"&gt;here&lt;/a&gt;. I also installed irqbalance so that I didn't have to worry too much about pinning the network card interrupts to specific CPUs (by default they are all pinned to the first CPU). This generally isn't the recommended way of doing it, &lt;a class="reference external" href="http://linuxjedi.co.uk/posts/2015/Jul/06/hitting-network-limits/"&gt;setting RPS&lt;/a&gt; is considered better, but worked fine for my testing.&lt;/p&gt;
&lt;p&gt;I used &lt;a class="reference external" href="http://software.es.net/iperf/"&gt;iperf3&lt;/a&gt; for measuring the bandwidth between the cards and did no tuning apart from setting up irqbalance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ iperf3 -Z -N -P4 -c ngx4
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The result summary showed almost exactly 3GBit/sec being transferred between the cards. This tallys with what was being observed using NGINX which was peaking at around 3.5GBit/sec with a little bit of tuning. The biggest problem with these cards and NGINX was the amount of connections-per-second. Whilst the Pro 1000 ET could easily handle the number of interrupts required for a high number of Packets Per Second, the Pro 1000 PT got very saturated quickly. Having a high number of PPS is vital to handling many connections per second with small amounts of data being transferred.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="gigabit-ethernet"&gt;
&lt;h2&gt;10 Gigabit Ethernet&lt;/h2&gt;
&lt;p&gt;I spent a while looking at alternative solutions for even faster networks. I looked into Inifiband, Dolphin Interconnect and other similar technologies. All designed for fast low-latency connections. I almost bought an Inifiband based solution when I stumbled across a pair of Mellanox ConnectX 2 10GbE adaptors. Ordering the pair of these came to £24.81 (plus £10.71 shipping from the US). They use SFP+ connectors so I bought a copper 1 meter SFP+ cable for £4.&lt;/p&gt;
&lt;p&gt;Mellanox ConnectX 2:&lt;/p&gt;
&lt;img alt="Mellanox ConnectX 2" src="/images/connectx.jpg" style="width: 400px;" /&gt;
&lt;p&gt;I plugged them in to each and set them up with static IPs and they worked great out-the-box. The Linux driver is showing 8 interrupts for the cards and irqbalance handles these nicely. With iperf executed as before the results are 9.4GBit/sec transferred through these cards.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;I have learned a lot from this, especially since this is my first physical exposure to 10GBit networking (I've configured software for them, just never wired them). The 4-port Intel NICs are amazing and I would highly recommend the Intel Pro 1000 ET or VT if you are in the market for a used card, the PT cards are good too but can't handle as many PPS.&lt;/p&gt;
&lt;p&gt;I'm extremely impressed by the performance of the 10GbE cards for such a low price and they would make great cards to do large file transfers (video work?) between machines in a home. I haven't yet found an affordable switch yet that has more than 2 SFP+ ports but I'm keeping my eye out for one. I'm also on the lookout for a cheap way of doing 40GbE connections but it may be a little too soon to do this kind of testing.&lt;/p&gt;
&lt;p&gt;I love the Lenovo ThinkStations and would have been equally happy with HP's Z-series, but couldn't find them at the right price-point. The Ivy-Bridge based S30 would actually make an amazing gaming machine if paired with a good graphics card. For me, they are both fantastic dev/test machines (and very fast at compiling). I've already used them to debug a lot of code and I'm looking forward to testing more hardware and software with them in the future.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="Nginx"></category><category term="networking"></category></entry><entry><title>Hitting Network Limits</title><link href="http://linuxjedi.co.uk/posts/2015/Jul/06/hitting-network-limits/" rel="alternate"></link><published>2015-07-06T18:10:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-07-06:posts/2015/Jul/06/hitting-network-limits/</id><summary type="html">&lt;p&gt;I recently wrote a high-level blog post for NGINX &lt;a class="reference external" href="https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/"&gt;outlining the new SO_REUSEPORT feature in NGINX&lt;/a&gt;. One of the problems I was having whilst doing the benchmarks for this was that I was hitting some kind of bottleneck before hitting the limit of what NGINX could process. This meant that it was difficult to get useful benchmarks.&lt;/p&gt;
&lt;p&gt;The main reason was down to hitting a limit on the maximum number of interrupts per second that could be processed. Despite the evidence staring me in the face it didn't click with me that all the interrupts were being processed on just one CPU.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PID   USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
3     root      20   0       0      0      0 R  95.4  0.0   0:10.28 ksoftirqd/0
21076 nobody    20   0   57208  31140   2272 S  26.5  1.0   9:41.90 nginx
21075 nobody    20   0   57164  31200   2376 S  26.2  1.0   9:47.15 nginx
21078 nobody    20   0   57236  31272   2376 S  25.8  1.0   9:40.71 nginx
21077 nobody    20   0   57184  31112   2268 R  25.2  1.0   9:40.94 nginx
&lt;/pre&gt;
&lt;p&gt;This is an example on my home test rig. The server is based on a Core2Quad CPU, nothing special but it helps pin bottlenecks a lot of the time. NGINX in this example has been configured with &lt;tt class="docutils literal"&gt;SO_REUSEPORT&lt;/tt&gt; enabled and we are returning just a very small packet response. The thing to notice here is that each NGINX worker is only using around 25-26% CPU (theoretically this machine has 400% CPU possible), and 95% CPU is being used for &lt;tt class="docutils literal"&gt;ksoftirqd&lt;/tt&gt;. This last part is the key, it basically means that CPU 0 is being used up nearly 100% dealing with network interrupts, hitting a limit of what the machine can process. Whilst I very quickly figured that network interrupts were the bottleneck the above didn't click with me at the time to be the issue.&lt;/p&gt;
&lt;p&gt;It turns out in Linux there is a way to spread these interrupts across all CPUs, I stumbled upon it by accident earlier today (and am kicking myself that I wasn't aware of it before). It is called &lt;strong&gt;Receive Packet Steering&lt;/strong&gt; (RPS). &lt;a class="reference external" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/network-rps.html"&gt;RedHat's Performance Tuning Guide&lt;/a&gt; has a good description of what it does and how to use it. To enable it on a quad-core machine simply do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;echo&lt;/span&gt; f &amp;gt; /sys/class/net/eth0/queues/rx-0/rps_cpus
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Obviously replacing &lt;em&gt;eth0&lt;/em&gt; with the name of your network card. Once enabled I was getting much better results:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PID   USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
21076 nobody    20   0   56140  29988   2272 R  93.8  1.0   9:12.14 nginx
21075 nobody    20   0   56152  30008   2376 R  92.9  1.0   9:17.54 nginx
21077 nobody    20   0   55996  29844   2268 R  92.4  1.0   9:11.67 nginx
21078 nobody    20   0   56160  30196   2376 R  91.7  1.0   9:11.85 nginx
&lt;/pre&gt;
&lt;p&gt;The client machine (which was also configured with this parameter) was reading 2.5x the traffic and nearly half the latency. In addition there is &lt;a class="reference external" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/network-rfs.html"&gt;Receive Flow Steering (RFS)&lt;/a&gt; which can reduce latency when RPS is used, in my testing the average latency was the same but the standard deviation was reduced.&lt;/p&gt;
&lt;p&gt;Network cards that can spread the hardware interrupts and stacks using DPDK or similar probably won't have this issue. But for cloud environments I suspect this option will help a great deal. As always &amp;quot;Your Mileage May Vary&amp;quot;.&lt;/p&gt;
&lt;p&gt;If you have any tips for increasing performance please feel free to leave them in the comments.&lt;/p&gt;
</summary><category term="Nginx"></category><category term="networking"></category></entry><entry><title>The LinuxJedi at NGINX</title><link href="http://linuxjedi.co.uk/posts/2015/Jun/26/the-linuxjedi-at-nginx/" rel="alternate"></link><published>2015-06-26T21:40:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-06-26:posts/2015/Jun/26/the-linuxjedi-at-nginx/</id><summary type="html">&lt;p&gt;Several months ago I was having a Sunday brunch with some friends in Seattle. One of the discussions that came up was the fact that &lt;a class="reference external" href="http://nginx.com/"&gt;NGINX&lt;/a&gt; needed to expand their developer relations team and we were thinking up names of likely candidates for the role. Fast forward to today and I've somehow wound up working at NGINX on the developer relations team!&lt;/p&gt;
&lt;p&gt;My role is &amp;quot;Senior Developer Advocate&amp;quot; and there are many parts of my role, but I guess it could be summarised as a cross between a community manager and community developer.&lt;/p&gt;
&lt;p&gt;I made many friends during my time at HP and it was hard leaving them, especially the Advanced Technology Group since this was the last team I worked in. Brian Aker has been my boss and mentor for many years through several companies and in particular he will always be a good friend.&lt;/p&gt;
&lt;div class="section" id="nginx-conference"&gt;
&lt;h2&gt;NGINX Conference&lt;/h2&gt;
&lt;p&gt;The day it became public that I was joining NGINX I got lots of people coming up to me telling me that NGINX had solved problems for them in the past. Even giving me thanks (which was a little strange because I hadn't even started yet).&lt;/p&gt;
&lt;p&gt;It seems everyone I meet has a unique and interesting story about their NGINX implemention. It is fantastic to work with a team that creates a tool that can be used in so many unique ways, many of which we could never imagine.&lt;/p&gt;
&lt;p&gt;This is where NGINX needs you, everyone has a unique story about NGINX and we want to give you the opportunity to tell yours. We have the &lt;a class="reference external" href="http://nginx.com/nginxconf/"&gt;NGINX Conference&lt;/a&gt; on September 22-24th 2015 and the CfP is still open for just under 1 week. So please come and submit your story!&lt;/p&gt;
&lt;p&gt;I will be at the conference and we have lots of fun and interesting things already planned.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nginx-so-far"&gt;
&lt;h2&gt;NGINX so far&lt;/h2&gt;
&lt;p&gt;The reason I haven't written a post in a while is NGINX has kept me so incredibly busy, but it has been a lot of fun too. That isn't to say I haven't been blogging, you can see a post I wrote called &lt;a class="reference external" href="http://nginx.com/blog/socket-sharding-nginx-release-1-9-1/"&gt;Socket Sharding in NGINX Release 1.9.1&lt;/a&gt; on the NGINX blog.&lt;/p&gt;
&lt;p&gt;I've flown to San Francisco and Moscow to meet the teams there. Everyone at NGINX is passionate about the product and it is a fantastic thing to see.&lt;/p&gt;
&lt;p&gt;I have also spoken to many members of the NGINX community as well as companies big and small who use NGINX as a part of their stack. All of them very enthusiastic about NGINX and excited to see what we have lined up for future releases (trust me, we have some amazing features lined up).&lt;/p&gt;
&lt;p&gt;There is a lot of work for me to do here but I'm enjoying every minute of it. I'm here to help the Open Source community and we have an awesome community, so lively that it is hard for me to keep up. I will try and take time out to blog more about things I'm working on at NGINX over the coming months.&lt;/p&gt;
&lt;p&gt;I look forward to the future of NGINX and seeing as many of you as I can at the &lt;a class="reference external" href="http://nginx.com/nginxconf/"&gt;NGINX Conference&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="Nginx"></category></entry><entry><title>Why Dependency Testing is Important</title><link href="http://linuxjedi.co.uk/posts/2015/Apr/23/why-dependency-testing-is-important/" rel="alternate"></link><published>2015-04-23T22:27:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-04-23:posts/2015/Apr/23/why-dependency-testing-is-important/</id><summary type="html">&lt;p&gt;Or...&lt;/p&gt;
&lt;div class="section" id="why-i-m-ditching-parallels"&gt;
&lt;h2&gt;Why I'm Ditching Parallels&lt;/h2&gt;
&lt;p&gt;I've been using Parallels quite successfully for several months now and I must admit it is a great way to run all my various Linux distros that I work with on my Mac.&lt;/p&gt;
&lt;p&gt;One of my main VMs is the latest Fedora release (currently Fedora 21). Fedora can be quite bleeding edge and in recent releases they have had a policy of a rolling kernel release inside the fixed twice-annual releases.&lt;/p&gt;
&lt;p&gt;A few weeks ago my Fedora VM ran an update which included an update to a 3.19 Kernel release. After a reboot my Fedora VM didn't start Xorg and was stuck at a service startup messages.&lt;/p&gt;
&lt;p&gt;I wasn't getting much response from Parallels support so I dug in to debug what had happened. It turns out that the Parallels tools had attempted to rebuild and failed. This meant that the required kernel modules were missing and a full boot could not happen. My Parallels had also updated so I could not switch to an older kernel, because this required a module rebuild and there development packages for the older kernels were not available.&lt;/p&gt;
&lt;p&gt;I ended up creating a quick temporary patch to fix this and &lt;a class="reference external" href="https://forum.parallels.com/threads/temporary-fix-for-kernel-3-19.328277/"&gt;posted it on Parallels forum&lt;/a&gt;. I eventually got a response from Parallels support stating they are working on the problem and to this day (now nearly 2 weeks after I created the patch) the problem has still not officially fixed by Parallels.&lt;/p&gt;
&lt;p&gt;Now, I understand that Parallels needs time to fix these things. But, this problem could have easily been resolved months ago, before it was even a problem to the users that have needed my patch.&lt;/p&gt;
&lt;p&gt;It comes down to a type of continuous integration that is relatively easy to set up. That is to test with new versions of your dependencies when they are upgraded.&lt;/p&gt;
&lt;p&gt;In the case of Linux this could be done by having a script that updates Arch Linux or similar daily, tries to install Parallels Tools and reports on failure. If Parallels had used this process with Arch Linux the problem would have been automatically detected in early February and they would have had time to fix it before it affected the bigger distros.&lt;/p&gt;
&lt;p&gt;This would cause a few false failures but those would be easy to weed out. They could even take a Fedora release and build a new kernel and other dependencies for it daily which will probably reduce the false failures.&lt;/p&gt;
&lt;p&gt;Today we are at a point where Ubuntu 15.04 has been released. The kernel with that distro is not compatible and there is no update to the proprietary Xorg driver that comes with Parallels for Xorg 1.17 that comes with this release. Making it completely unusable with Parallels.&lt;/p&gt;
&lt;p&gt;I need to use virtual machines for my day job at Nginx as I did in HP, so this problem is a real productivity killer and has meant I've had to finally give up and ditch Parallels. Today I've been trying VMWare Fusion instead and so far it is working great with every distro I need to use, even with updated kernel and Xorg.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="Testing"></category><category term="Parallels"></category></entry><entry><title>Variable Length Arrays in C</title><link href="http://linuxjedi.co.uk/posts/2015/Feb/24/variable-length-arrays-in-c/" rel="alternate"></link><published>2015-02-24T13:25:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-02-24:posts/2015/Feb/24/variable-length-arrays-in-c/</id><summary type="html">&lt;p&gt;In a bid to aim this blog at multiple levels I'm going to talk today about Variable Length Arrays (VLAs) in C.  I'm covering this topic in particular because a friend who is a Harvard CS50 student recently asked me about them.&lt;/p&gt;
&lt;p&gt;As many people know, C is my favourite programming language to work in (several have questioned my sanity thanks to this).  It isn't always the right programming language for every project and where it is appropriate I will use an alternative language.  But it is my favourite to work with and HP's Advanced Technology Group allows me to do a lot of work in it.&lt;/p&gt;
&lt;p&gt;In a previous post on my old blog &lt;a class="reference external" href="http://thelinuxjedi.blogspot.co.uk/2014/02/why-vlais-is-bad.html"&gt;I discussed why VLAIS (Variable Length Arrays In Structs) is a bad idea&lt;/a&gt;.  Put simply this is a GCC only feature which can cause a lot of problems.  Unfortunately they are used a lot in the Linux kernel but this is being improved as part of the work to port the kernel to work with the CLang compiler.&lt;/p&gt;
&lt;p&gt;VLAs themselves are not as bad.  For those who don't know what they are, here is an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;argc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[])&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;fred&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;printname&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// 4 chars + nul terminator&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;printname&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="n"&gt;name_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="n"&gt;memcpy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name_copy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Name is: %s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name_copy&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The bit I'm referring to is &amp;quot;&lt;tt class="docutils literal"&gt;char name_copy[length];&lt;/tt&gt;&amp;quot;.  Normally array sizes are fixed at compile time but with the C99 standard VLAs were introduced.  They were in several compilers before that but it was not a standard.  The compiler will turn this into code which allocates &lt;tt class="docutils literal"&gt;name_copy&lt;/tt&gt; to the correct size when the function is entered and frees the memory when the function returns.&lt;/p&gt;
&lt;p&gt;This is incredibly useful but is not without problems:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;You need to make sure that the application doesn't try to use &lt;tt class="docutils literal"&gt;free()&lt;/tt&gt; on the VLA.  This will likely end very badly&lt;/li&gt;
&lt;li&gt;If you have a pointer to a VLA you need to make sure you don't try to access it when the function has returned (just like accessing memory that has been freed)&lt;/li&gt;
&lt;li&gt;Many implementations allocate this memory on the stack instead of the main pool of memory&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This last one could be a problem because most platforms at least have a soft limit on stack size.  The default for the machine I'm typing on for example is 8MiB.  This sounds a lot but with recursion it is possible to easily blow this, especially with large arrays.  Ideally VLAs should be used just for allocations you know are going to be quite small and predictable.  If it is a user input without sanitation you could be opening yourself up to an attack vector.&lt;/p&gt;
&lt;p&gt;In summary, VLAs are useful, they make allocation and freeing much easier, especially for multi-dimension arrays.  But you need to be very careful using them and only for small allocations.&lt;/p&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="C"></category></entry><entry><title>Live Kernel Patching - Why You Should NOT Use It</title><link href="http://linuxjedi.co.uk/posts/2015/Feb/14/live-kernel-patching-why-you-should-not-use-it/" rel="alternate"></link><published>2015-02-14T23:11:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-02-14:posts/2015/Feb/14/live-kernel-patching-why-you-should-not-use-it/</id><summary type="html">&lt;p&gt;Just under a year ago on my old blog I &lt;a class="reference external" href="http://thelinuxjedi.blogspot.co.uk/2014/03/live-kernel-patching.html"&gt;discussed&lt;/a&gt; and even &lt;a class="reference external" href="http://thelinuxjedi.blogspot.co.uk/2014/06/live-kernel-patching-video-demo.html"&gt;demoed&lt;/a&gt; the new Linux live kernel patching solutions.  I was reviewing these technologies out of my own curiosity as well as HP's Advanced Technology Group having an interest.  I think these technologies are great, I am personally more of a fan of the user experience of RedHat's kpatch solution but any solution is a great technical achievement.&lt;/p&gt;
&lt;p&gt;Having said this I believe that the use case for this technology is quite narrow.  Last time I looked into these technologies only patches that affected the code of functions could be modified.  Changing structs and data definitely didn't work and I suspect that changing function declarations was also dangerous.  There is also a performance overhead.  If you are replacing functions that are called many times a second the additional overhead of a jump to the replaced function will cause a performance hit.&lt;/p&gt;
&lt;p&gt;Where I can see live patching to be useful is on desktop machines that need critical patching but rebooting will cause losses in productivity or in science/academia where there are often long-running applications that cannot be paused for a reboot.&lt;/p&gt;
&lt;p&gt;The only place where I do not think it should be used and unfortunately I fear it will be the main place it is used is for Internet and network services.  Everyone should strive for 100% uptime, this is something I do not dispute.  But the 100% uptime should be for the &lt;em&gt;application&lt;/em&gt;, not necessarily the underlying hardware.&lt;/p&gt;
&lt;p&gt;100% uptime of hardware is not a possible reality.  Moving parts such as fans and hard disks fail, components degrade over time, power can fail along with redundancies and at some point you may need to upgrade or move hardware.  The solution to this is the same as the solution to kernel upgrades: multiple active servers.&lt;/p&gt;
&lt;p&gt;For web servers, have multiple servers load balanced, you can have multiple load balancers too.  Even multiple active MySQL servers are possible thanks to technologies such as &lt;a class="reference external" href="http://galeracluster.com/"&gt;Galera Cluster&lt;/a&gt;.  When the kernel needs updating you can apply a rolling reboot.  Technologies such as &lt;a class="reference external" href="http://www.ansible.com/"&gt;Ansible&lt;/a&gt; will even help you do this by making sure only a certain amount of servers from each tier of your architecture is updated at any time.&lt;/p&gt;
&lt;p&gt;I commend SUSE and RedHat in helping to replace a technology that otherwise was long since dead in the Open Source world.  Oracle seemingly killing the development of the Open Source KSplice was an annoyance to many.  But in today's world, is a technology we really need?&lt;/p&gt;
&lt;p&gt;I would be interested to hear in the comments if there is a use case I have missed.&lt;/p&gt;
&lt;div class="section" id="update"&gt;
&lt;h2&gt;Update&lt;/h2&gt;
&lt;p&gt;2015-02-14 23:11&lt;/p&gt;
&lt;p&gt;Robert Collins pointed out on Facebook that the overhead for the jump is likely to be too minimal to cause a performance issue and anything that is called too often is not worth patching (I am inclined to agree).&lt;/p&gt;
&lt;p&gt;Robert also points out that there is a cost involved in migrating many VMs when the hosts need upgrading for a zero-day.  I sort-of agree with this scenario.  For public cloud this is a problem, although I believe typically the rolling upgrade is performed by pausing VMs and resuming rather than migration.  With public cloud I feel this is acceptable with a short notice period.&lt;/p&gt;
&lt;p&gt;For private cloud there are ways around this issue, typically VMs are throw-away in private cloud so it is easy to just blow them away and recreate them in an already updated part of the data centre rather than migrate.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="Kernel"></category></entry><entry><title>Why The C10K Problem is Important</title><link href="http://linuxjedi.co.uk/posts/2015/Feb/14/why-the-c10k-problem-is-important/" rel="alternate"></link><published>2015-02-14T17:49:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-02-14:posts/2015/Feb/14/why-the-c10k-problem-is-important/</id><summary type="html">&lt;p&gt;For those who are not familiar with the concept the &lt;a class="reference external" href="http://www.kegel.com/c10k.html"&gt;C10K problem&lt;/a&gt; is about trying to handle many simultaneous clients on a web server.  Originally written about trying to handle 10,000 clients on a Gigabit Ethernet connection back in the late 90s, but this is still a problem today (with more clients and bigger connections).&lt;/p&gt;
&lt;p&gt;Websites are dealing with more and more traffic as not only more people every year are using the internet with more devices but they are also increasing their usage.  By this I mean not only in the amount they use the internet but higher resolution monitors and faster CPUs for interactive content means the amount of data for websites has increased too.  It has got to the point where dozens of files are needed to load a site such as Facebook or Twitter.&lt;/p&gt;
&lt;p&gt;Faster and faster internet connections have also meant that users are far more impatient.  I first used the Internet at home in 1995 with a 33.6Kbit/sec modem.  Websites would typically take at least 10 seconds to access and load, sometimes more.  This didn't bother us back then.  A decade later Internet speeds are much faster and sites are much bigger (javascript, css, high-res/retina images, flash, etc...), but users expect load times of only a couple of seconds and access/wait times of less than a second, even on cell phones.&lt;/p&gt;
&lt;p&gt;Although the server hardware is evolving it is difficult to keep up with client demand and thus the C10K problem is still an issue today.  One way of solving this is to have many servers or cloud instances handling the traffic.  This can cause a cost issue for many companies, especially startups.  The other solution which is gaining in popularity is event driven I/O.&lt;/p&gt;
&lt;p&gt;Typically with event driven I/O you run the server single threaded, although you can offload onto multiple threads/processes.  A single event loop is run which listens for new events such as data in or out and reacts based on the event.  In Linux this relies heavily in &lt;tt class="docutils literal"&gt;epoll()&lt;/tt&gt; and BSD based distributions &lt;tt class="docutils literal"&gt;kqueue()&lt;/tt&gt;.  These are edge-triggered &lt;tt class="docutils literal"&gt;poll()&lt;/tt&gt; replacements which notifies the application of a transition from a socket not-ready to ready.&lt;/p&gt;
&lt;p&gt;It does not feel natural to use a single-threaded solution in a world of multi-core CPUs but there is a wealth of evidence that this approach works very well.  It is in-general simpler to code since many of the complications of dealing with many threads such as race conditions and locking go away.  But there is the caveat that handling an event incorrectly could cause an application to hang.  The event-driven approach also typically reduces CPU usage of the application which on modern hardware can help with energy and heat savings.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://nginx.com/"&gt;Nginx&lt;/a&gt; is an example of a web/proxy server which uses event-driven design such as this.  Part of the reason Nginx is becoming so popular is that the design of it gives it the ability to scale much easier than more traditional web servers such as Apache.&lt;/p&gt;
&lt;p&gt;With &lt;a class="reference external" href="http://libattachsql.org/"&gt;libAttachSQL&lt;/a&gt; we at the Advanced Technology Group are doing something very similar, but this time on the client side.  So that client applications can handle many connections to one or more servers on a single thread with ease.  We already have plans to add even more features around this for the 2.0 release.&lt;/p&gt;
&lt;p&gt;Internally libAttachSQL uses &lt;a class="reference external" href="https://nikhilm.github.io/uvbook/introduction.html"&gt;libuv&lt;/a&gt; which is a high performance event driven I/O library born out of Node.js.  It also uses the event driven features of OpenSSL for SSL connections.&lt;/p&gt;
&lt;p&gt;I have heard many times that C10K is not an issue today due to the high performance hardware at much lower prices than were previously available.  It may not be a 10K problem any more, but it is still an issue.  Even the big boys of the Internet today are using event driven solutions to help them scale and keep the running costs low.  This is evident by the increased usage of technologies such as Nginx.&lt;/p&gt;
&lt;p&gt;In the last couple of years there is talk of the &lt;a class="reference external" href="http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html"&gt;C10M problem&lt;/a&gt; which suggests moving control of the TCP/IP stack completely away from the kernel to the user layer.  Personally I feel this is going a little too far for generic applications.  It could mean that your applications require specific OS/kernel and even specific hardware to run.  For people writing web servers and libraries this can easily bloat the code and by the time you have dealt with edge cases I suspect the performance increase will not be worth the extra work.&lt;/p&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="Nginx"></category><category term="libAttachSQL"></category></entry><entry><title>YubiKey NEO with GnuPG</title><link href="http://linuxjedi.co.uk/posts/2015/Jan/28/yubikey-neo-with-gnupg/" rel="alternate"></link><published>2015-01-28T10:24:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-01-28:posts/2015/Jan/28/yubikey-neo-with-gnupg/</id><summary type="html">&lt;p&gt;Last week I published a blog post on using &lt;a class="reference external" href="http://linuxjedi.co.uk/posts/2015/Jan/19/yubikey-for-os-logins/"&gt;YubiKey for OS Logins&lt;/a&gt;.  Since then I've had a request from inside HP to create a blog post on using the YubiKey NEO with GnuPG which is another thing I have done with my NEO.&lt;/p&gt;
&lt;p&gt;The YubiKey NEO has a built-in CCID smartcard interface which is disabled by default.  GnuPG can use this instead of a passphrase for your keys.  This blog post will give some indication on how to do this.  It is assuming you have some experience of using gpg, I wanted to keep this as short as possible and it can be a complex topic.  As with any process I highly recommend backing up your existing gpg keys if you have any:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gpg -o key-backup.key --armour --export-secret-keys email@address.com
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Before we start you will need to enable CCID on the YubiKey NEO, it is disabled by default.  The easiest way to do this is to use the &lt;a class="reference external" href="https://developers.yubico.com/yubikey-neo-manager/"&gt;YubiKey NEO Manager&lt;/a&gt;.  In this tool click on &lt;em&gt;Change connection mode&lt;/em&gt; and tick the CCID box.&lt;/p&gt;
&lt;p&gt;First of all the YubiKey's CCID needs initializing.  The following enables the admin commands and starts the generation of new keys.  If you want to use the YubiKey with your old key then Ctrl-C out when it asks about the expiry.  Otherwise go through the prompts and make sure to use the &lt;tt class="docutils literal"&gt;save&lt;/tt&gt; command afterwards:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gpg --card-edit

gpg/card&amp;gt; admin

gpg/card&amp;gt; generate
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, every time you use it you will be asked for the PIN or admin PIN.  These are set to some basic defaults ('123456' and '12345678' respectively) so it would be a good idea to change these:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gpg --change-pin
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Option 1 is to change the access PIN and option 3 is to change the admin PIN.&lt;/p&gt;
&lt;div class="section" id="using-an-existing-key"&gt;
&lt;h2&gt;Using an existing key&lt;/h2&gt;
&lt;p&gt;If you have followed the steps above you have a primed smart card on your YubiKey ready to load your key onto.  Before we do this you need a total of three subkeys, one for encryption (you likely already have), one for signing and one for authentication.  We need to enter the key editor mode of gpg to look at this and add these:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gpg --expert --edit-key email@address.com
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should get a list of your key and subkeys straight away.  The &lt;em&gt;Usage&lt;/em&gt; column is the one we are looking for here.  You need an &lt;tt class="docutils literal"&gt;E&lt;/tt&gt; for Encryption, &lt;tt class="docutils literal"&gt;A&lt;/tt&gt; for Authentication and &lt;tt class="docutils literal"&gt;S&lt;/tt&gt; for Signing.  If you are missing any of these:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gpg&amp;gt; addkey
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Select option 8 (RSA) and then add whatever you need.  Repeat the process until you have one of each.&lt;/p&gt;
&lt;p&gt;You are now ready to move the keys to the YubiKey:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gpg&amp;gt; key 1
gpg&amp;gt; keytocard
gpg&amp;gt; key 2
gpg&amp;gt; keytocard
gpg&amp;gt; key 3
gpg&amp;gt; keytocard
gpg&amp;gt; save
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Your secret subkeys are now on the YubiKey which can be verified using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gpg --card-status
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
</summary><category term="YubiKey"></category><category term="Security"></category><category term="HP"></category><category term="Advanced Technology Group"></category></entry><entry><title>Working With Git</title><link href="http://linuxjedi.co.uk/posts/2015/Jan/25/working-with-git/" rel="alternate"></link><published>2015-01-25T16:33:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-01-25:posts/2015/Jan/25/working-with-git/</id><summary type="html">&lt;p&gt;At a recent meeting HP's Advanced Technology Group has been agreeing on standards for working with git, covering mainly the collaboration and versioning aspects.  Today I will share with you how I do this in my Open Source projects which has some crossover with the group's work.&lt;/p&gt;
&lt;div class="section" id="versioning"&gt;
&lt;h2&gt;Versioning&lt;/h2&gt;
&lt;p&gt;I use &lt;a class="reference external" href="http://semver.org/"&gt;Semantic Versioning&lt;/a&gt; everywhere I can.  In fact I used to pretty much use it everywhere before I knew it was a standard.  This means that I have my GitHub trees layed out so that 'master' is the latest stable code, v1.0 branch is the latest stable v1.0 code, v1.1 branch is the latest in v1.1 and so on.  I then create GPG signed tags from these branches to create releases as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git checkout v1.1
$ git tag -s v1.1.2 -m &lt;span class="s1"&gt;&amp;#39;Version 1.1.2 release&amp;#39;&lt;/span&gt;
$ git push --tags
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="forking"&gt;
&lt;h2&gt;Forking&lt;/h2&gt;
&lt;p&gt;Even if it is just me working on a particular project I will always use forks and pull requests to work on code.  This model works well with &lt;a class="reference external" href="http://semver.org/"&gt;Travis CI&lt;/a&gt; because it can test code prior to merging and give feedback in the pull request.&lt;/p&gt;
&lt;p&gt;For this example I'm assuming you are using SSH keys and two-factor authentication with Git, if you aren't you need to do this ASAP.  Instructions can be found on &lt;a class="reference external" href="https://github.com/blog/1614-two-factor-authentication"&gt;GitHub's blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A fork is typically created as follows:&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;Create a fork in GitHub, you can do this by clicking the fork button on a project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Grab a local copy of your fork to work with (replacing &lt;cite&gt;USERNAME&lt;/cite&gt; and &lt;cite&gt;Repository&lt;/cite&gt; with whatever is applicable to you):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone git@github.com:USERNAME/Repository
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Then you should set the upstream remote so that you can easily grab the latest code as needed.  I've used HTTPS here because you don't need to be authenticated to do this unless it is a private repository:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git remote add upstream https://github.com/PROJECT/Repository
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="starting-a-new-branch"&gt;
&lt;h2&gt;Starting a New Branch&lt;/h2&gt;
&lt;p&gt;Whenever you are starting a new group of work, create a new branch.  This holds true for features or just basic one line fixes.  In your fork:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git checkout master
$ git pull --ff-only upstream master
$ git push
$ git checkout -b my_feature
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This pulls the latest code from the upstream master to your master, pushes that to your fork and then creates a new checkout based on that code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="syncing-fork"&gt;
&lt;h2&gt;Syncing Fork&lt;/h2&gt;
&lt;p&gt;If you have worked on some code and at the same time someone else has merged code that may conflict (a pull request will tell you this straight away) you can merge upstream with your commits as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git fetch upstream
$ git merge upstream/master
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This fetches the upstream code into a local cache and then will merge it.  If there are conflicts they will be flagged for you to resolve.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pull-requests"&gt;
&lt;h2&gt;Pull Requests&lt;/h2&gt;
&lt;p&gt;When your code is good and ready you can send it up as a pull request.  To do this you first need to push it up to your local repository:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git push --set-upstream origin my_feature
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then when you go to the project's repository you will see a button to file a pull request.  If you are using continuous integration such as Travis CI you should wait for that to give a green result and preferably peer review too before clocking the merge button.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="Git"></category></entry><entry><title>C Library Visibility</title><link href="http://linuxjedi.co.uk/posts/2015/Jan/22/c-library-visibility/" rel="alternate"></link><published>2015-01-22T09:25:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-01-22:posts/2015/Jan/22/c-library-visibility/</id><summary type="html">&lt;p&gt;I was surprised by the recent &lt;a class="reference external" href="http://mysqlserverteam.com/how-to-use-ssl-and-mysql-client-library-in-the-same-binary/"&gt;announcement&lt;/a&gt; that MySQL are going to start to conceal the hidden function calls in their C connector.  Surprised because although this is great news I had expected them to do this years ago.  Working for HP's Advanced Technology Group I realise I take such things for granted.  For this blog post I'm going to talk about why it is important and how to do it.&lt;/p&gt;
&lt;p&gt;So, when you create a dynamic library in C the default thing that happens is every function call in that library effectively becomes a potential API call.  Whether you document every single function or not to make it official API is up to you but I suspect in 99.99% of cases there are private functions you don't want users to mess with.  Additionally holding the symbol information for every function so that you can link your application to it takes a massive amount of space, one such library I can think of is 8x bigger than it should be due to exposing every function call.&lt;/p&gt;
&lt;p&gt;In MySQL's case and likely others this can cause a problem with collisions during linking.  MySQL can use its bundled in YaSSL library to supply SSL, and due to the functions being exposed this can cause problems if your application links to libmysqlclient and OpenSSL since they both use the same public API calls in many places.&lt;/p&gt;
&lt;p&gt;RedHat and other distributions actually solve this in MySQL by stripping the binaries of unneeded symbols at compile time.  This is indeed one effective solution.  But I don't believe this is the correct solution.  In fact Ulrich Drepper in &lt;a class="reference external" href="https://software.intel.com/sites/default/files/m/a/1/e/dsohowto.pdf"&gt;How To Write Shared Libraries&lt;/a&gt; pretty much reserves this as a last resort.&lt;/p&gt;
&lt;div class="section" id="link-time-visibility"&gt;
&lt;h2&gt;Link Time Visibility&lt;/h2&gt;
&lt;p&gt;The solution I and many others recommend is using visibility at link time.  There are three parts to applying this:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;The compiler flag&lt;/li&gt;
&lt;li&gt;An extra include file&lt;/li&gt;
&lt;li&gt;Marking the functions you want to be in the API as public&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I shall go through these steps for GCC, other compilers will be very similar and there is plenty of information on the internet about this.&lt;/p&gt;
&lt;div class="section" id="compiler-flag"&gt;
&lt;h3&gt;Compiler Flag&lt;/h3&gt;
&lt;p&gt;You simply need to add one compiler flag which will hide all function calls by default instead of exposing them all by default:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
-fvisibility=hidden
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="include-file"&gt;
&lt;h3&gt;Include File&lt;/h3&gt;
&lt;p&gt;This is one example taken from &lt;a class="reference external" href="http://libattachsql.org/"&gt;libAttachSQL&lt;/a&gt; and there is an example with more platform support on the &lt;a class="reference external" href="https://gcc.gnu.org/wiki/Visibility"&gt;GCC Visibility manual page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this example you need to change &lt;tt class="docutils literal"&gt;BUILDING_ASQL&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;ASQL_API&lt;/tt&gt; to suit your own naming.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;#if defined(BUILDING_ASQL)&lt;/span&gt;
&lt;span class="cp"&gt;# if defined(HAVE_VISIBILITY) &amp;amp;&amp;amp; HAVE_VISIBILITY&lt;/span&gt;
&lt;span class="cp"&gt;#  define ASQL_API __attribute__ ((visibility(&amp;quot;default&amp;quot;)))&lt;/span&gt;
&lt;span class="cp"&gt;# elif defined (__SUNPRO_C) &amp;amp;&amp;amp; (__SUNPRO_C &amp;gt;= 0x550)&lt;/span&gt;
&lt;span class="cp"&gt;#  define ASQL_API __global&lt;/span&gt;
&lt;span class="cp"&gt;# elif defined(_MSC_VER)&lt;/span&gt;
&lt;span class="cp"&gt;#  define ASQL_API extern __declspec(dllexport)&lt;/span&gt;
&lt;span class="cp"&gt;# else&lt;/span&gt;
&lt;span class="cp"&gt;#  define ASQL_API&lt;/span&gt;
&lt;span class="cp"&gt;# endif &lt;/span&gt;&lt;span class="cm"&gt;/* defined(HAVE_VISIBILITY) */&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;#else  &lt;/span&gt;&lt;span class="cm"&gt;/* defined(BUILDING_ASQL) */&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;# if defined(_MSC_VER)&lt;/span&gt;
&lt;span class="cp"&gt;#  define ASQL_API extern __declspec(dllimport)&lt;/span&gt;
&lt;span class="cp"&gt;# else&lt;/span&gt;
&lt;span class="cp"&gt;#  define ASQL_API&lt;/span&gt;
&lt;span class="cp"&gt;# endif &lt;/span&gt;&lt;span class="cm"&gt;/* defined(_MSC_VER) */&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;#endif &lt;/span&gt;&lt;span class="cm"&gt;/* defined(BUILDING_ASQL) */&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="marking-public-api"&gt;
&lt;h3&gt;Marking Public API&lt;/h3&gt;
&lt;p&gt;With the above include file you need to define &lt;tt class="docutils literal"&gt;BUILDING_ASQL&lt;/tt&gt; somewhere in your library compiling.  This makes sure that during the library compiling/linking all the functions can be seen but they will be hidden at link time from external applications.&lt;/p&gt;
&lt;p&gt;Then when you are defining an API call in your .h file you can do as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;hidden_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="n"&gt;ASQL_API&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;public_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can see here I have prepended the function declaration with &lt;tt class="docutils literal"&gt;ASQL_API&lt;/tt&gt; for any function I wish to be part of the public API.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;If you leave undocumented API dangling people will tend to use it, which causes all sorts of issues when you want to change some of the internal functions.  Ideally when writing an API setting the visibility should be a very early step, but thankfully it is one that can easily be added to a project at any time.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="C"></category><category term="API"></category><category term="MySQL"></category></entry><entry><title>Webserver Concealing</title><link href="http://linuxjedi.co.uk/posts/2015/Jan/21/webserver-concealing/" rel="alternate"></link><published>2015-01-21T19:30:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-01-21:posts/2015/Jan/21/webserver-concealing/</id><summary type="html">&lt;p&gt;Right now there are bots on the internet scanning every IP possible for vulnerable servers.  This is a fact of life on the internet.  This means you need to keep any internet facing machines as secure as possible.&lt;/p&gt;
&lt;p&gt;Whilst it is no panacea one step you can take to hide the version of the web server software you are using.  If there is a zero-day bug a bot or malicious person is scanning for and you are vulnerable this can help hide it.&lt;/p&gt;
&lt;p&gt;I was recently looking at an SSL problem with a local high school and found the server was reporting the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Server:Apache/2.2.15 (Novell)
X-Powered-By:PHP/5.3.3
&lt;/pre&gt;
&lt;p&gt;There are several problems here, all are solvable:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Novell is unmaintained.  There are no security updates.  This needs replacing with something such as a Linux distribution or FreeBSD which does get updates.&lt;/li&gt;
&lt;li&gt;Apache 2.2.15 and PHP 5.3.3 are both around 4-5 years old.  They have dozens of known bugs and secrity flaws which are fixed in newer versions.&lt;/li&gt;
&lt;li&gt;The server version is advertised.  I was able to find this information easily which means any hacker can, then he can cross-reference with known security flaws and do whatever he wants with this server.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Clearly the first thing that should be done here is to update the server and its software packages.  But in addition it is possible to hide the versions used so at least one of these three points is eliminated.&lt;/p&gt;
&lt;p&gt;First to hide the Apache server version we simply need to add one line to the configuration file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;ServerTokens&lt;/span&gt; prod
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The server will now only announce that it is running Apache but no server version or OS used.&lt;/p&gt;
&lt;p&gt;As for PHP, you can turn the announcement off completely with the following in your php.ini file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="na"&gt;expose_php&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;off&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is only one small security improvement, but having many layers of security will help dramatically to protect your servers.  I only recommend this after dealing with things like software updates, firewalls, code security audits, etc...&lt;/p&gt;
&lt;p&gt;As a side note, if you are using PHP I highly recommend using &lt;a class="reference external" href="http://suhosin.org/"&gt;Suhosin&lt;/a&gt; to help secure your server installation.&lt;/p&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="Security"></category></entry><entry><title>The Pointer Corruption Bug</title><link href="http://linuxjedi.co.uk/posts/2015/Jan/20/the-pointer-corruption-bug/" rel="alternate"></link><published>2015-01-20T09:16:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-01-20:posts/2015/Jan/20/the-pointer-corruption-bug/</id><summary type="html">&lt;p&gt;Or an alternative name for this post...&lt;/p&gt;
&lt;div class="section" id="why-api-docs-should-have-examples"&gt;
&lt;h2&gt;Why API Docs Should Have Examples&lt;/h2&gt;
&lt;p&gt;As part of my continuation of libAttachSQL for HP's Advanced Technology Group I have recently been focusing on a Python based wrapper called pyAttachSQL.  This is currently at an alpha level of release with no package builds yet.&lt;/p&gt;
&lt;p&gt;Today I want to talk about one (silly on my part) very frustrating bug I found whilst working on pyAttachSQL and why this means API docs should have examples for every call.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-crash"&gt;
&lt;h2&gt;The Crash&lt;/h2&gt;
&lt;p&gt;Whilst writing the group connection functions I was using a &lt;a class="reference external" href="https://docs.python.org/2/c-api/arg.html#c.Py_BuildValue"&gt;Py_BuildValue&lt;/a&gt; call to generate parameters to use in a callback &lt;a class="reference external" href="https://docs.python.org/2/c-api/object.html#c.PyObject_CallObject"&gt;PyObject_CallObject&lt;/a&gt;.  So the code looked a little like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cbargs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Py_BuildValue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;iOOO&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;events&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;pycon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;pycon&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;cb_args&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;PyObject_CallObject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;cb_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cbargs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;Py_DECREF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cbargs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Those who are Python API veterans will be able to see straight away where I went wrong but I am quite new to the API.  The code was segfaulting on &lt;tt class="docutils literal"&gt;PyObject_CallObject&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="my-big-dumb-mistake"&gt;
&lt;h2&gt;My Big Dumb Mistake&lt;/h2&gt;
&lt;img alt="" src="/images/droids.jpg" /&gt;
&lt;p&gt;Whilst debugging this I found the problem was in &lt;tt class="docutils literal"&gt;cbargs&lt;/tt&gt;, for some reason the pointers to &lt;tt class="docutils literal"&gt;pycon&lt;/tt&gt; and alike were slightly different to when they were set.  After some time going over it again and again in GDB and watching the pointers get incremented it suddenly hit me.  The incrementation was happening during reference increment functions inside &lt;tt class="docutils literal"&gt;Py_BuildValue&lt;/tt&gt;.  Which makes sense because the one first items in a &lt;tt class="docutils literal"&gt;PyObject&lt;/tt&gt; structure is the reference count.  I was supposed to pass pointers, not pointers to pointers.  The &lt;tt class="docutils literal"&gt;Py_BuildValue&lt;/tt&gt; function has no type checking so was taking whatever you passed to it as a pointer to a structure.&lt;/p&gt;
&lt;p&gt;So the question many of you would be asking is: why did you pass pointers to pointers?  That is easy to answer...  Earlier in the code I have been using the &lt;a class="reference external" href="https://docs.python.org/2/c-api/arg.html#c.PyArg_ParseTuple"&gt;PyArg_ParseTuple&lt;/a&gt; and similar functions which are on the same documentation page, using a similar API and I assumed the API was consistent with no examples to show me otherwise.  The fix was to simply remove the &lt;tt class="docutils literal"&gt;&amp;amp;&lt;/tt&gt; symbols from the above code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lessons-learnt"&gt;
&lt;h2&gt;Lessons Learnt&lt;/h2&gt;
&lt;p&gt;I guess there is two lessons I have learnt from this:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Don't assume that an API is consistent&lt;/li&gt;
&lt;li&gt;Add examples for every API call in the documentation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I'll shortly be opening a ticket for libAttachSQL and pyAttachSQL to implement the second item.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="C"></category><category term="Python"></category><category term="API"></category><category term="Documentation"></category><category term="libAttachSQL"></category></entry><entry><title>YubiKey for OS Logins</title><link href="http://linuxjedi.co.uk/posts/2015/Jan/19/yubikey-for-os-logins/" rel="alternate"></link><published>2015-01-19T16:14:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-01-19:posts/2015/Jan/19/yubikey-for-os-logins/</id><summary type="html">&lt;p&gt;I have mentioned a few times in this blog that HP takes security very seriously and HP's Advanced Technology Group is always looking into new ways of making things secure.  Recently the team all got a &lt;a class="reference external" href="https://www.yubico.com/products/yubikey-hardware/yubikey-neo/"&gt;YubiKey Neo&lt;/a&gt; to use.  The initial idea was we would be trying the FIDO U2F with Google accounts but several of us went much further.&lt;/p&gt;
&lt;p&gt;Yazz Atlas from our team has been working on getting his SSH key into the NEO with some success.  I initially got my GPG key to work with the smart card feature in the NEO and have since been tinkering with a couple of other things.&lt;/p&gt;
&lt;p&gt;I happened to find an original YubiKey in a drawer which was used for OTP two-factor authentication to a server I no longer have access to.  I wanted to use this as a way of two-factor authentication for my computers.  Unfortunately there is no good way of doing this on the Mac at the moment so I came up with a different way of doing a less secure two-factor authentication with it (but more secure than a fixed password).&lt;/p&gt;
&lt;p&gt;The original YubiKey's have two &amp;quot;slots&amp;quot; in them.  Each slot can store either an OTP two-factor authentication identity or a static password.  You can tap a YubiKey to get the first slot and hold for around 3 seconds for the second slot.  The way I'm using this is to have the slots store static passwords, I then have a hand typed part of my password and a second static part stored on the YubiKey.  This means that if the YubiKey is stolen/used it is useless without my hand typed-part.  I have a backup of the YubiKey's static password in my &lt;a class="reference external" href="https://lastpass.com/"&gt;LastPass&lt;/a&gt; account (which incidentally uses two-factor authentication).&lt;/p&gt;
&lt;p&gt;There was a minor snag in this to begin with, my Macs are encrypted with FileVault which has a known login problem.  If you type a password too quickly it actually drops some of characters that you have typed.  The YubiKey is a virtual keyboard and although it deliberately doesn't type too quickly, it is too fast for FileVault.  If your YubiKey is a version 2.3 or higher this is easy to fix.  In the &lt;a class="reference external" href="https://www.yubico.com/products/services-software/personalization-tools/"&gt;YubiKey Personalization Tool&lt;/a&gt; you can go to settings and change the output frequency, this setting change can then be used to update a slot.  But if like me your original YubiKey is an older version you cannot do this.  There is a way around this by generating a new key, but it requires the command line (the GUI doesn't have the options to change the delay whilst generating a new key).&lt;/p&gt;
&lt;p&gt;First of all you need the command line YubiKey Personalization Tool.  There are &lt;a class="reference external" href="https://yubico.github.io/yubikey-personalization/releases.html"&gt;Linux, Mac and Windows versions available&lt;/a&gt; and some Linux distros have it in their repository.  Using this tool you can generate a new random static password with a typing delay as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ykpersonalize -2 -opacing-20ms -ostrong-pw2 -ostrong-pw1 -ostatic-token
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can change the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-2&lt;/span&gt;&lt;/tt&gt; to &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-1&lt;/span&gt;&lt;/tt&gt; if you wish to program slot 1 instead.  The typing delay is added using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;pacing-20ms&lt;/span&gt;&lt;/tt&gt; option.  Unfortunately this flag doesn't quite match the GUI, if you want to match the GUI options here is what you need to use:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;20ms Delay - &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;pacing-10ms&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;40ms Delay - &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;pacing-20ms&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;60ms Delay - &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;pacing-20ms&lt;/span&gt;&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;pacing-10ms&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For FileVault it is recommended you using the 40ms delay as a minimum.  The 20ms delay works most of the time but can still fail.&lt;/p&gt;
&lt;p&gt;So, now to log into my machines I type in the part of the password I have remembered and then press the YubiKey to fill in the rest of the password.&lt;/p&gt;
</summary><category term="YubiKey"></category><category term="Security"></category><category term="HP"></category><category term="Advanced Technology Group"></category></entry><entry><title>USB Flash Drives - Trimming the FAT</title><link href="http://linuxjedi.co.uk/posts/2015/Jan/05/usb-flash-drives-trimming-the-fat/" rel="alternate"></link><published>2015-01-05T22:34:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-01-05:posts/2015/Jan/05/usb-flash-drives-trimming-the-fat/</id><summary type="html">&lt;p&gt;As with most of you who read this blog I carry USB flash drives around with me all the time.  Right now I have 3 Kingston DTSE9 sticks on my keyring of various sizes each with a different purpose.  Whilst these drives are nowhere near the fastest out there they are the only ones I have had so far that don't snap off keyrings.&lt;/p&gt;
&lt;img alt="" src="/images/king16GB_DTSE9_2.jpg" /&gt;
&lt;p&gt;For this blog post I'll be talking about data where security is not a priority.  My encrypted flash drives are currently using &lt;a class="reference external" href="https://veracrypt.codeplex.com/"&gt;VeraCrypt&lt;/a&gt; but that is beyond the scope of this blog post.&lt;/p&gt;
&lt;div class="section" id="fat32-woes"&gt;
&lt;h2&gt;FAT32 woes&lt;/h2&gt;
&lt;p&gt;The largest one I have is 64GB and due to some of the work I do for HP's Advanced Technology Group this often needs to have large files on it.  Traditionally FAT32 has been used as a file system for memory cards and flash drives, one of the biggest reasons for this is that it is compatible with pretty much every computer operating system out there.  But FAT32 has many flaws:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;it has a single file limit of 4GB.  That may sound huge but this rules out DVD images and large databases.&lt;/li&gt;
&lt;li&gt;it does not support POSIX based permissions which anyone who uses Linux is used to.&lt;/li&gt;
&lt;li&gt;it doesn't support journalling so with a removable drive data corruption is very common.&lt;/li&gt;
&lt;li&gt;as a minor issue it is also designed for spinning disks with many more write cycles than flash.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What I needed was something that fixes as many of those problems as possible but is also compatible with Linux, Mac OS X and Windows.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="alternatives"&gt;
&lt;h2&gt;Alternatives&lt;/h2&gt;
&lt;p&gt;I looked into several alternatives to FAT32 and summarised my findings below.  I'm primarily looking for out-of-box support, I know there are paid third-party add-ons to operating systems to add support.&lt;/p&gt;
&lt;div class="section" id="exfat"&gt;
&lt;h3&gt;exFAT&lt;/h3&gt;
&lt;p&gt;A company looked into all the problems with FAT and created a filesystem designed for use with flash drives, they called this &lt;em&gt;exFAT&lt;/em&gt;.  Unfortunately that company is Microsoft and what they created is not an open standard and is full of patents.&lt;/p&gt;
&lt;p&gt;This means that only people who have paid licenses can use exFAT.  This includes Mac OS X and digital camera manufacturers but unfortunately means Linux support is very limited.&lt;/p&gt;
&lt;p&gt;exFAT is therefore thrown out.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="hfs"&gt;
&lt;h3&gt;HFS+&lt;/h3&gt;
&lt;p&gt;HFS+ is the primary file system used by Mac OS X.  There is good support for this in Macs (obviously) and Linux, but no support in Windows.&lt;/p&gt;
&lt;p&gt;Unfortunately that means HFS+ is out.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ntfs"&gt;
&lt;h3&gt;NTFS&lt;/h3&gt;
&lt;p&gt;NTFS is Microsoft's primary file system in NT based operating systems (for most people this means Windows XP onwards).  Like HFS+ it isn't a bad file system for flash drives and in recent Linux distributions has very good support.  Unfortunately in Mac OS X it can only be used in read-only mode.&lt;/p&gt;
&lt;p&gt;Another Microsoft FS thrown out.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="udf"&gt;
&lt;h3&gt;UDF&lt;/h3&gt;
&lt;p&gt;Yes, you read that correctly, UDF.  It is a filesystem which was originally designed for use with optical media.  But, it has since been adapted for use with hard drives and flash drives!&lt;/p&gt;
&lt;p&gt;The maximum file size is 16EB (actually bigger than the maximum volume size).  It supports POSIX file system permissions.  But most importantly, it works with Linux, Mac OS X and Windows (Vista onwards) out of the box!&lt;/p&gt;
&lt;p&gt;In addition the UDF format was designed for packet writing so it works by appending on the end of data on the file system and expiring the old data.  In theory this could lead to less wear of the drive.  Flash drives typically use dynamic wear leveling which is similar to the static wear leveling used in SSDs but less complex.  The algorithm used may mean that the packet writing has no real advantage on the wear of the drive.  I don't have enough data to say for certain.&lt;/p&gt;
&lt;p&gt;The file system itself works like a journal.  It appends new data to the end of the log with a new version of the file table.  So, if a write was not completed successfully it will use the previous version of the log.  This also means recovery of deleted files is possible by traversing previous versions of the data log.&lt;/p&gt;
&lt;p&gt;For me this ticks all the boxes so I am using it with a 64GB UDF formatted flash.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="making-a-udf-flash-drive"&gt;
&lt;h2&gt;Making a UDF flash drive&lt;/h2&gt;
&lt;div class="section" id="mac-os-x"&gt;
&lt;h3&gt;Mac OS X&lt;/h3&gt;
&lt;p&gt;Unfortunately Disk Utility doesn't let you format a flash drive as UDF but you can use the command line to do it.&lt;/p&gt;
&lt;p&gt;First of all you need to figure out the drive path for your flash drive.  It will be in the format &lt;tt class="docutils literal"&gt;/dev/disk{drive_no}&lt;/tt&gt; where drive_no is the drive number, if it is followed by the letter &lt;em&gt;s&lt;/em&gt; and another number then that is a partition and not what we need at this stage:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil list
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you need to find out the block size (it is typically 512).  Make a note of this number because you will need it later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil info /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;Block Size&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The drive can't be changed until we unmount the partitions so run this for every partition that is currently in-use for your drive:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil unmount /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt;s&lt;span class="o"&gt;{&lt;/span&gt;partition_no&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Due to the nature of the UDF format it is possible that the operating system would still detect the drive as FAT32 afterwards so we need to blank the drive with zeros.  This could take some time:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil secureErase &lt;span class="m"&gt;0&lt;/span&gt; /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now the drive can be formatted, replace &lt;em&gt;block_size&lt;/em&gt; with the number you wrote down above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo newfs_udf -b &lt;span class="o"&gt;{&lt;/span&gt;block_size&lt;span class="o"&gt;}&lt;/span&gt; /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally the drive can be mounted again for use as normal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil mount /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="linux"&gt;
&lt;h3&gt;Linux&lt;/h3&gt;
&lt;p&gt;In Linux things get a little easier.  First of all unmount the partitions on the drive and then we need the block size, write this one down:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo blockdev --getbsz /dev/sd&lt;span class="o"&gt;{&lt;/span&gt;drive_letter&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We then need to zero out the drive so that it isn't incorrectly detected:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo dd &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/dev/zero &lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/dev/sd&lt;span class="o"&gt;{&lt;/span&gt;drive_letter&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="nv"&gt;bs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1M &lt;span class="nv"&gt;count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we need to make the UDF format, replacing &lt;em&gt;block_size&lt;/em&gt; with the number noted above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo mkudffs -b &lt;span class="o"&gt;{&lt;/span&gt;block_size&lt;span class="o"&gt;}&lt;/span&gt; --media-type&lt;span class="o"&gt;=&lt;/span&gt;hd /dev/sd&lt;span class="o"&gt;{&lt;/span&gt;drive_letter&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="USB"></category><category term="Flash"></category><category term="Filesystems"></category><category term="HP"></category><category term="Advanced Technology Group"></category></entry><entry><title>Family Travel Technology</title><link href="http://linuxjedi.co.uk/posts/2014/Dec/28/family-travel-technology/" rel="alternate"></link><published>2014-12-28T16:18:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-12-28:posts/2014/Dec/28/family-travel-technology/</id><summary type="html">&lt;p&gt;My family and I spent a few days before Christmas visiting family away from home.  We booked two interconnecting hotel rooms so our children could have their own room and we could sleep with a little bit of peace.  As with many trips there is often a period between dinner and sleep where you are stuck in a hotel room with nothing to do so this time I devised a plan for entertainment.&lt;/p&gt;
&lt;p&gt;First of all, we all have internet connected devices, between phones, gaming devices, my laptop, etc... we actually had 9 devices with us (some I will talk about later in this post).  As with many hotel WiFi plans there is a cap on the number of devices you can have assigned to your room.  The WiFi itself was quite speedy for a hotel and was fast enough to share across several devices.  To get around this limit and also to create our own personal network I used a &lt;a class="reference external" href="http://www.netgear.co.uk/home/products/networking/wifi-routers/PR2000.aspx"&gt;Netgear PR2000 Travel Router&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="" src="/images/netgear.jpg" /&gt;
&lt;p&gt;The Netgear router connects to a public network via. a network cable or WiFi and creates its own WiFi/wired network for you to use.  This means you use any one of your devices to log into the hotel WiFi and to the hotel it looks like just the router is connected.  This is a fantastic device and I benchmarked it before I went on vacation, I was easily pushing 20mbit over the internet in both directions with it in less than ideal conditions so it is was perfect for travelling with.  I absolutely love this device and I think the only flaw I have found with it is that it gets into a reboot loop after you first configure it and have to power cycle it to make it work.&lt;/p&gt;
&lt;p&gt;The only thing I would like to see improved with this device is the inclusion of a US power connector.  It can be powered via. USB or by clipping on an included EU or UK power plug.  If you power it using a wall plug connection you can use the USB port on the device to share a USB stick or drive across your private network.  That said I've tested and a C7 (figure of 8) power cable will be compatible if not elegant to look at, much like using a C7 in an Apple Mac power adaptor.&lt;/p&gt;
&lt;p&gt;As part of the entertainment we wanted a movie night.  The children had a selection of films they wanted to watch and I had already transcoded them from their DVDs onto my laptop.  My wife and I are currently watching box sets of the TV series of 24 and we wanted to watch this.  So, in the children's room I hooked my Apple TV into their TV and in our room I used the HDMI on my Macbook Pro (2014 13&amp;quot; retina model) in our room.  The Macbook Pro does not have an included DVD drive, luckily earlier this year I bought a USB &lt;a class="reference external" href="https://www.samsung.com/uk/consumer/memory-cards-hdd-odd/odd/odd/SE-S084D/TSBS"&gt;Samsung S084D DVD drive&lt;/a&gt; second hand for £4.&lt;/p&gt;
&lt;p&gt;To stream the video to the Apple TV in the children's room I used an application called &lt;a class="reference external" href="http://beamer-app.com/"&gt;Beamer&lt;/a&gt;.  This converted the video on-the-fly to a format the Apple TV could use and streamed it.  It has worked great with every file format I have thrown at it and automatically find the Apple TV with no problems.  With the time slider on it I could also see how long was left on the film before they had finished with it.  Whilst in the parents room we were using Apple's built-in DVD player.&lt;/p&gt;
&lt;p&gt;The whole setup worked perfectly, throughout the stay the children watched two films on the Apple TV using Beamer and we got half way through a series of 24.  Being my geeky self I'm impressed at how well the setup glued together mostly with parts I already had at home.  The whole lot fitted easily in my backpack and I had lots of space to spare for other things.&lt;/p&gt;
</summary><category term="Apple"></category><category term="Netgear"></category><category term="Beamer"></category></entry><entry><title>Autotools and Mac Universal binaries</title><link href="http://linuxjedi.co.uk/posts/2014/Nov/25/autotools-and-mac-universal-binaries/" rel="alternate"></link><published>2014-11-25T16:38:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-11-25:posts/2014/Nov/25/autotools-and-mac-universal-binaries/</id><summary type="html">&lt;p&gt;Recently I have been working on a Python based wrapper for &lt;a class="reference external" href="http://libattachsql.org/"&gt;libAttachSQL&lt;/a&gt; and found that when testing on a Mac I was having trouble compiling the wrapper.  It turns out that Python included in Mac operating systems uses a universal binary (also called fat binary) format and since libAttachSQL is not compiled that way it would not link correctly.&lt;/p&gt;
&lt;p&gt;For those who have never come across this, Universal binaries were originally intended to contain executables for multiple platforms (such as PPC and i386) to ease hardware transition.  The OS will only load the compatible part into memory and use that.  Python as well as several other current Mac binaries are compiled to have i386 and x86_64 binaries in one package.&lt;/p&gt;
&lt;p&gt;Compiling a Universal binary is actually relatively easy but I didn't want to put the hard work on the use who is compiling the library and I wanted something I could use in other projects in the future.  So I have created an m4 script which can be used with Autotools to build Universal binaries.  This can be found in my &lt;a class="reference external" href="https://github.com/LinuxJedi/m4scripts"&gt;m4 scripts GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The script in question is called &lt;tt class="docutils literal"&gt;ax_mac_universal.m4&lt;/tt&gt; and when the &lt;tt class="docutils literal"&gt;AX_UNIVERSAL_BINARY&lt;/tt&gt; macro is used it will automatically detect if the environment supports universal binaries and add the necessary compiler flags to build them.  It also adds &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--enable-universal-binary&lt;/span&gt;&lt;/tt&gt; to configure so that you can force this feature on/off at will.&lt;/p&gt;
&lt;p&gt;This script will be included as part of the upcoming libAttachSQL 1.0.2 release.&lt;/p&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="libAttachSQL"></category><category term="Autotools"></category></entry><entry><title>Why JSON is bad for applications</title><link href="http://linuxjedi.co.uk/posts/2014/Oct/31/why-json-is-bad-for-applications/" rel="alternate"></link><published>2014-10-31T20:39:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-10-31:posts/2014/Oct/31/why-json-is-bad-for-applications/</id><summary type="html">&lt;p&gt;Today I read an article about how company X has improved things by amongst other things ditching JSON after 2 years of using it.  Before I start on this subject I should say that JSON does have its place.  If you have a web application where a browser is talking to a web server and in particular uses JavaScript then JSON is a good fit.&lt;/p&gt;
&lt;p&gt;I've discussed this issue several times before with &lt;a class="reference external" href="http://krow.net/"&gt;Brian Aker&lt;/a&gt; who works with me at HP's Advanced Technology Group and in the past I have been hit with the issues I'm going to talk about here.&lt;/p&gt;
&lt;p&gt;JSON is human readable and easy to parse, that cannot be denied and for prototyping is good in a pinch.  The first problem comes when you need to validate data.  I've been stung many times by one end trying to read/write the JSON in a slightly different format to the other end, the end result is always not pretty.  This is one advantage that XML and SOAP has going for it over JSON since validation is easier.  I'm personally not a fan of XML but there are many who are.&lt;/p&gt;
&lt;p&gt;There are additional problems when you start using mobile platforms.  Mobile networks are unreliable, you may have a good 3G signal but it is possible to only get dial-up speed through it due to all the other users.  JSON is verbose, XML more so which requires more data transfer.  Whilst this can be resolved with protocol compression it will require additional decoding on the client side to do this.  In addition data conversion will be needed in many cases for numeric fields.&lt;/p&gt;
&lt;p&gt;The biggest problem with JSON is versioning.  As you add more features to your application there will likely come a time where you need to change the data structure for your messages.  Often you can't guarantee that your client is using the same version of the software as your server so backwards and forwards compatibility problems can arise.  Resolving these often makes the JSON messages very complex to create and decode.  This is not as much of a problem for web applications because the browser usually grabs an update version of the JavaScript on execution.  So changing the data format at any time is easy as long as both ends agree on the format.&lt;/p&gt;
&lt;div class="section" id="the-solution"&gt;
&lt;h2&gt;The solution&lt;/h2&gt;
&lt;p&gt;For many applications the data you are sending is directly from a database or at least data that has been modified since being read from a database.  So you will likely want the data model for your messages to match this as much as possible.  This is where &lt;a class="reference external" href="https://developers.google.com/protocol-buffers/"&gt;Google's Protocol Buffers&lt;/a&gt; fit nicely.&lt;/p&gt;
&lt;p&gt;Protocol Buffers allow you to specify a schema for the data in a human readable format, it actually looks a little like a database schema.  They will automatically validate the data for you and have versioning built-in.  This means you can make your code easily backwards and forwards compatible.&lt;/p&gt;
&lt;p&gt;There is a positive and negative side to the data transfer of Protocol Buffers.  It is a binary protocol.  This means it takes up minimal bandwidth on the wire but also means that it is not human readable and difficult to figure out which data is for which field (although should not be used for security through obscurity).  The same could be said if you were given InnoDB table data without the schemas.  It also means it may be possible to compress the data further with something like LZO or DEFLATE.&lt;/p&gt;
&lt;p&gt;I recommend application developers consider Protocol Buffers instead of JSON when they are next developing a server/client application.&lt;/p&gt;
&lt;div class="note"&gt;
&lt;p class="first admonition-title"&gt;Note&lt;/p&gt;
&lt;p class="last"&gt;I updated this article to explain the binary protocol a little better.  Thanks to Antony Curtis for pointing it out.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category></entry><entry><title>libAttchSQL Hits First GA!</title><link href="http://linuxjedi.co.uk/posts/2014/Oct/22/libattchsql-hits-first-ga/" rel="alternate"></link><published>2014-10-22T11:35:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-10-22:posts/2014/Oct/22/libattchsql-hits-first-ga/</id><summary type="html">&lt;p&gt;We have come a long way since the first code was put down for &lt;a class="reference external" href="http://libattachsql.org"&gt;libAttachSQL&lt;/a&gt; on the 4th July.  It has been a fantastic project to work on so I am very pleased to announce our first GA release.&lt;/p&gt;
&lt;p&gt;For those who haven't seen it so far libAttachSQL is a non-blocking, lightweight C API for MySQL servers.  It is Apache 2.0 licensed so is compatible with most Open Source and commercial licensing.  HP's Advanced Technology Group saw a need in this field not just for HP itself but for other companies and projects too.&lt;/p&gt;
&lt;p&gt;As for the GA release itself, there are not many changes over the RC release beyond stability fixes.  A full list can be seen in the &lt;a class="reference external" href="http://docs.libattachsql.org/en/latest/appendix/version_history.html#version-1-0"&gt;version history documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition to the GA release we have recently had a driver for &lt;a class="reference external" href="https://launchpad.net/sysbench"&gt;Sysbench&lt;/a&gt; merged into their trunk so libAttachSQL can be used for benchmarking MySQL servers.  We have also started work on a tool called &lt;a class="reference external" href="https://github.com/libattachsql/attachbench"&gt;AttachBench&lt;/a&gt; which when complete will run similar MySQL tests as Sysbench but will allow for multiple connections per thread (something libAttachSQL excels at).  At the moment AttachBench requires the tables from Sysbench's &amp;quot;Select&amp;quot; test already setup and I don't recommend tinkering with it yet unless you don't mind getting a bit dirty.&lt;/p&gt;
&lt;p&gt;With the release of libAttachSQL 1.0.0 we have also launched a new website on &lt;a class="reference external" href="http://libattachsql.org"&gt;libattachsql.org&lt;/a&gt;.  It is a basic Pelican based site (very much like this blog) but will make it much easier for anyone to add content, just like this blog all the source is in &lt;a class="reference external" href="https://github.com/libattachsql/libattachsql.org"&gt;RST files on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Download links for libAttachSQL 1.0.0 can be found on the &lt;a class="reference external" href="http://libattachsql.org/posts/2014/Oct/21/version-100-ga-released/"&gt;News section&lt;/a&gt; of the project website.  There is a source package as well as packages for RHEL/CentOS 6.x and 7.x.  Packages for Ubuntu 12.04 and 14.04 are waiting to be built in the PPA at time of posting.  We hope to have releases for more operating systems in the near future.&lt;/p&gt;
&lt;p&gt;Rest assured we are not stopping here.  I already have ideas of what I want to see in 1.1 and we have some spin-off projects planned.  If you would like to learn more please come along to my talk on libAttachSQL at &lt;a class="reference external" href="http://www.percona.com/live/london-2014/sessions/libattachsql-next-generation-c-connector-mysql"&gt;Percona Live London&lt;/a&gt;.  I'm also talking to several people outside of HP to see what they would like in libAttachSQL and am happy to talk to anyone else who wants to know more and has feedback.&lt;/p&gt;
&lt;p&gt;Many thanks to everyone who has helped us get this far.&lt;/p&gt;
</summary><category term="MySQL"></category><category term="libAttachSQL"></category><category term="HP"></category><category term="Advanced Technology Group"></category></entry><entry><title>libAttachSQL Single Thread vs. libmysqlclient Multi Thread</title><link href="http://linuxjedi.co.uk/posts/2014/Oct/15/libattachsql-single-thread-vs-libmysqlclient-multi-thread/" rel="alternate"></link><published>2014-10-15T20:01:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-10-15:posts/2014/Oct/15/libattachsql-single-thread-vs-libmysqlclient-multi-thread/</id><summary type="html">&lt;p&gt;I have recently &lt;a class="reference external" href="http://linuxjedi.co.uk/posts/2014/Oct/03/libattachsql-benchmarks-with-sysbench/"&gt;posted up benchmarks&lt;/a&gt; of &lt;a class="reference external" href="http://libattachsql.org"&gt;libAttachSQL&lt;/a&gt; vs. libmysqlclient using sysbench.  Whilst these are great and shows the results I hoped for, this isn't what we designed libAttachSQL for.  It was designed for non-blocking many connections per thread.&lt;/p&gt;
&lt;p&gt;With this in mind I spent today knocking up a quick benchmark tool which replicates the Sysbench &amp;quot;Select&amp;quot; test but using libAttachSQL's connection groups on a single thread.  The code for this can be seen in the new &lt;a class="reference external" href="https://github.com/libattachsql/attachbench"&gt;AttachBench&lt;/a&gt; GitHub tree.  Of course the secondary reason for this is to try and hammer the connection groups feature, which of course did find a bug when I scaled to around 32 connections.  This has been fixed in libAttachSQL's master ready for the next release and is what I am using for these benchmarks.&lt;/p&gt;
&lt;div class="section" id="the-test"&gt;
&lt;h2&gt;The Test&lt;/h2&gt;
&lt;p&gt;I used the exact same test rig and configuration as the previous Sysbench tests and as before the test was run with 1,000,000 queries.  The AttachBench tool executed was as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ attachbench --db&lt;span class="o"&gt;=&lt;/span&gt;sbtest --user&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;test&lt;/span&gt; --pass&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;test&lt;/span&gt; --db&lt;span class="o"&gt;=&lt;/span&gt;testdb --queries&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1000000&lt;/span&gt; --connections&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;32&lt;/span&gt; --host&lt;span class="o"&gt;=&lt;/span&gt;/tmp/mysql.sock --port&lt;span class="o"&gt;=&lt;/span&gt;0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I've added the results to the previous select test to the graph for comparison.  The two &amp;quot;Select&amp;quot; result sets are for Sysbench with one connection per thread.  The third is with AttachBench running the same queries, just with many connections in a single thread.&lt;/p&gt;
&lt;img alt="" src="/images/select_single_thread.png" /&gt;
&lt;p&gt;This exceeded my expectations.  Having many connections in a single thread actually outperforms many threads with one connection each.  It is early days and there is much more testing and improvement that can be done.  But I'm very encouraged by these results.&lt;/p&gt;
&lt;p&gt;I'll be talking more about libAttachSQL and these results at &lt;a class="reference external" href="http://www.percona.com/live/london-2014/sessions/libattachsql-next-generation-c-connector-mysql"&gt;Percona Live London next month&lt;/a&gt; so please come along if you are in the area.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="MySQL"></category><category term="libAttachSQL"></category><category term="HP"></category><category term="Advanced Technology Group"></category></entry><entry><title>libAttachSQL 0.9.0 RC - Connection Groups</title><link href="http://linuxjedi.co.uk/posts/2014/Oct/14/libattachsql-090-rc-connection-groups/" rel="alternate"></link><published>2014-10-14T17:14:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-10-14:posts/2014/Oct/14/libattachsql-090-rc-connection-groups/</id><summary type="html">&lt;p&gt;It has been just over 4 months since I started working on &lt;a class="reference external" href="http://libattachsql.org"&gt;libAttachSQL&lt;/a&gt; for HP's Advanced Technology Group.  Today marks the first (and hopefully only) RC release of the library.&lt;/p&gt;
&lt;div class="section" id="connection-groups"&gt;
&lt;h2&gt;Connection Groups&lt;/h2&gt;
&lt;p&gt;The only real new feature that has been added to 0.9.0 is the concept of connection groups which is something I'm pretty excited about.  Internally libAttachSQL uses event loops to supply the non-blocking API.  Connection Groups join a bunch of connections together into a group that uses a single event loop.  This makes things much more efficient internally and makes applications easier to code too.&lt;/p&gt;
&lt;p&gt;Here is a simplified example of how to use it (for a more detailed example see our &lt;a class="reference external" href="http://docs.libattachsql.org/en/latest/api/examples.html#group-conncetions"&gt;example in the documentation&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;First we need to create the group and add connections to it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;attachsql_group_create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;attachsql_connect_create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3306&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;testdb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;attachsql_group_add_connection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;attachsql_connect_set_callback&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;callbk&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;con_no&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;con&lt;/tt&gt; array is just an array of connection objects and &lt;tt class="docutils literal"&gt;con_no&lt;/tt&gt; is just an array of integers so that the callback that I'll show shortly knows which connection number it is (only useful for displaying in this example).  The last three lines there will be repeated multiple times with different array numbers to add connections.&lt;/p&gt;
&lt;p&gt;We now send queries to the connections:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;attachsql_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strlen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;query1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;attachsql_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strlen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;query2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And so forth until we have sent a query to all the connections we want.&lt;/p&gt;
&lt;p&gt;Finally we want to run the connection group until complete:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;done_count&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;attachsql_group_run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this example &lt;tt class="docutils literal"&gt;done_count&lt;/tt&gt; is simply a global integer which increments as each callback hits EOF.  You could conceivably run various other parts of your application here and then call &lt;tt class="docutils literal"&gt;attachsql_group_run()&lt;/tt&gt; again when ready.&lt;/p&gt;
&lt;p&gt;I'm going to paste the whole callback here because it should be mostly self-explanatory, it is called when an event occurs and the code reacts to the event:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;callbk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;attachsql_connect_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;current_con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attachsql_events_t&lt;/span&gt; &lt;span class="n"&gt;events&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attachsql_error_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;uint8_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;con_no&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;uint8_t&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="n"&gt;attachsql_query_row_st&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kt"&gt;uint16_t&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="k"&gt;switch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;events&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nl"&gt;ATTACHSQL_EVENT_CONNECTED&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Connected event on con %d&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;con_no&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nl"&gt;ATTACHSQL_EVENT_ERROR&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Error exists on con %d: %d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;con_no&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attachsql_error_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
      &lt;span class="n"&gt;attachsql_error_free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nl"&gt;ATTACHSQL_EVENT_EOF&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Connection %d finished&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;con_no&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="n"&gt;done_count&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
      &lt;span class="n"&gt;attachsql_query_close&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_con&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nl"&gt;ATTACHSQL_EVENT_ROW_READY&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;attachsql_query_row_get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;attachsql_query_column_count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_con&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Con: %d, Column: %d, Length: %zu, Data: %.*s &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;con_no&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="n"&gt;attachsql_query_row_next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_con&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nl"&gt;ATTACHSQL_EVENT_NONE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The EOF call happens when we reach the end of the result set.  You could easily make this a job server here sending more queries when the previous queries are complete.  ROW_READY should be familiar to anyone who has seen previous examples of libAttachSQL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-and-packages"&gt;
&lt;h2&gt;Release and Packages&lt;/h2&gt;
&lt;p&gt;libAttachSQL 0.9.0RC is out today, there is a source release as well as packages for RHEL &amp;amp; CentOS 6/7 64bit as well as an Ubuntu PPA 12.04/14.04 32bit and 64bit.  Links to all these can be found on the news section of the &lt;a class="reference external" href="http://libattachsql.org/"&gt;libAttachSQL site&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="MySQL"></category><category term="libAttachSQL"></category><category term="HP"></category><category term="Advanced Technology Group"></category></entry></feed>