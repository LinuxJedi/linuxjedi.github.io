<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>LinuxJedi's /dev/null</title><link href="http://linuxjedi.co.uk/" rel="alternate"></link><link href="http://linuxjedi.co.uk/feeds/general.atom.xml" rel="self"></link><id>http://linuxjedi.co.uk/</id><updated>2014-11-25T16:38:00+00:00</updated><entry><title>Autotools and Mac Universal binaries</title><link href="http://linuxjedi.co.uk/posts/2014/Nov/25/autotools-and-mac-universal-binaries/" rel="alternate"></link><updated>2014-11-25T16:38:00+00:00</updated><author><name></name></author><id>tag:linuxjedi.co.uk,2014-11-25:posts/2014/Nov/25/autotools-and-mac-universal-binaries/</id><summary type="html">&lt;p&gt;Recently I have been working on a Python based wrapper for &lt;a class="reference external" href="http://libattachsql.org/"&gt;libAttachSQL&lt;/a&gt; and found that when testing on a Mac I was having trouble compiling the wrapper.  It turns out that Python included in Mac operating systems uses a universal binary (also called fat binary) format and since libAttachSQL is not compiled that way it would not link correctly.&lt;/p&gt;
&lt;p&gt;For those who have never come across this, Universal binaries were originally intended to contain executables for multiple platforms (such as PPC and i386) to ease hardware transition.  The OS will only load the compatible part into memory and use that.  Python as well as several other current Mac binaries are compiled to have i386 and x86_64 binaries in one package.&lt;/p&gt;
&lt;p&gt;Compiling a Universal binary is actually relatively easy but I didn't want to put the hard work on the use who is compiling the library and I wanted something I could use in other projects in the future.  So I have created an m4 script which can be used with Autotools to build Universal binaries.  This can be found in my &lt;a class="reference external" href="https://github.com/LinuxJedi/m4scripts"&gt;m4 scripts GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The script in question is called &lt;tt class="docutils literal"&gt;ax_mac_universal.m4&lt;/tt&gt; and when the &lt;tt class="docutils literal"&gt;AX_UNIVERSAL_BINARY&lt;/tt&gt; macro is used it will automatically detect if the environment supports universal binaries and add the necessary compiler flags to build them.  It also adds &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--enable-universal-binary&lt;/span&gt;&lt;/tt&gt; to configure so that you can force this feature on/off at will.&lt;/p&gt;
&lt;p&gt;This script will be included as part of the upcoming libAttachSQL 1.0.2 release.&lt;/p&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="libAttachSQL"></category><category term="Autotools"></category></entry><entry><title>Why JSON is bad for applications</title><link href="http://linuxjedi.co.uk/posts/2014/Oct/31/why-json-is-bad-for-applications/" rel="alternate"></link><updated>2014-10-31T20:39:00+00:00</updated><author><name></name></author><id>tag:linuxjedi.co.uk,2014-10-31:posts/2014/Oct/31/why-json-is-bad-for-applications/</id><summary type="html">&lt;p&gt;Today I read an article about how company X has improved things by amongst other things ditching JSON after 2 years of using it.  Before I start on this subject I should say that JSON does have its place.  If you have a web application where a browser is talking to a web server and in particular uses JavaScript then JSON is a good fit.&lt;/p&gt;
&lt;p&gt;I've discussed this issue several times before with &lt;a class="reference external" href="http://krow.net/"&gt;Brian Aker&lt;/a&gt; who works with me at HP's Advanced Technology Group and in the past I have been hit with the issues I'm going to talk about here.&lt;/p&gt;
&lt;p&gt;JSON is human readable and easy to parse, that cannot be denied and for prototyping is good in a pinch.  The first problem comes when you need to validate data.  I've been stung many times by one end trying to read/write the JSON in a slightly different format to the other end, the end result is always not pretty.  This is one advantage that XML and SOAP has going for it over JSON since validation is easier.  I'm personally not a fan of XML but there are many who are.&lt;/p&gt;
&lt;p&gt;There are additional problems when you start using mobile platforms.  Mobile networks are unreliable, you may have a good 3G signal but it is possible to only get dial-up speed through it due to all the other users.  JSON is verbose, XML more so which requires more data transfer.  Whilst this can be resolved with protocol compression it will require additional decoding on the client side to do this.  In addition data conversion will be needed in many cases for numeric fields.&lt;/p&gt;
&lt;p&gt;The biggest problem with JSON is versioning.  As you add more features to your application there will likely come a time where you need to change the data structure for your messages.  Often you can't guarantee that your client is using the same version of the software as your server so backwards and forwards compatibility problems can arise.  Resolving these often makes the JSON messages very complex to create and decode.  This is not as much of a problem for web applications because the browser usually grabs an update version of the JavaScript on execution.  So changing the data format at any time is easy as long as both ends agree on the format.&lt;/p&gt;
&lt;div class="section" id="the-solution"&gt;
&lt;h2&gt;The solution&lt;/h2&gt;
&lt;p&gt;For many applications the data you are sending is directly from a database or at least data that has been modified since being read from a database.  So you will likely want the data model for your messages to match this as much as possible.  This is where &lt;a class="reference external" href="https://developers.google.com/protocol-buffers/"&gt;Google's Protocol Buffers&lt;/a&gt; fit nicely.&lt;/p&gt;
&lt;p&gt;Protocol Buffers allow you to specify a schema for the data in a human readable format, it actually looks a little like a database schema.  They will automatically validate the data for you and have versioning built-in.  This means you can make your code easily backwards and forwards compatible.&lt;/p&gt;
&lt;p&gt;There is a positive and negative side to the data transfer of Protocol Buffers.  It is a binary protocol.  This means it takes up minimal bandwidth on the wire but also means that it is not human readable and difficult to figure out which data is for which field (although should not be used for security through obscurity).  The same could be said if you were given InnoDB table data without the schemas.  It also means it may be possible to compress the data further with something like LZO or DEFLATE.&lt;/p&gt;
&lt;p&gt;I recommend application developers consider Protocol Buffers instead of JSON when they are next developing a server/client application.&lt;/p&gt;
&lt;div class="note"&gt;
&lt;p class="first admonition-title"&gt;Note&lt;/p&gt;
&lt;p class="last"&gt;I updated this article to explain the binary protocol a little better.  Thanks to Antony Curtis for pointing it out.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category></entry><entry><title>Blogging Platforms</title><link href="http://linuxjedi.co.uk/posts/2014/Oct/04/blogging-platforms/" rel="alternate"></link><updated>2014-10-04T22:16:00+01:00</updated><author><name></name></author><id>tag:linuxjedi.co.uk,2014-10-04:posts/2014/Oct/04/blogging-platforms/</id><summary type="html">&lt;p&gt;A couple of weeks ago I ditched Blogger as my main blogging platform.  The main reason for this was the editing tools were breaking posts containing code.  Whilst it is a great platform for basic blogging, it is not suitable for developers blogs.&lt;/p&gt;
&lt;p&gt;So, I was on the hunt for blogging platforms that would make it easy for me to write posts that contain technical content and is not expensive to run.  I also don't want to be maintaining my own web server, I may be capable of doing this but I don't want the time overhead.&lt;/p&gt;
&lt;p&gt;I tried several things out that met some of my requirements but many didn't fit all.  Wordpress was probably the closest, but I had trouble bending the free templates to my will.&lt;/p&gt;
&lt;p&gt;With many on my team at HP's Advanced Technology Group using &lt;a class="reference external" href="https://pages.github.com/"&gt;GitHub Pages&lt;/a&gt; for blog posts I thought I would give it a try.  Most of the team are trying out Jekyll which looks really good, but isn't for me.  I prefer &lt;a class="reference external" href="http://docutils.sourceforge.net/rst.html"&gt;reStructuredText&lt;/a&gt; to Markdown and use it every day for the &lt;a class="reference external" href="http://docs.libattachsql.org"&gt;libAttachSQL documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On my journey I came across &lt;a class="reference external" href="http://tinkerer.me/"&gt;Tinkerer&lt;/a&gt; which is a layer on top of &lt;a class="reference external" href="http://sphinx-doc.org/"&gt;Python Sphinx&lt;/a&gt; to generate a blog site from RST files.  This was great for me because Sphinx is the renderer used for libAttachSQL's docs both in the build system and &lt;a class="reference external" href="https://readthedocs.org/"&gt;Read The Docs&lt;/a&gt;.  I created a new blog on this hosted on GitHub Pages and &lt;a class="reference external" href="https://disqus.com/"&gt;Disqus&lt;/a&gt; for comments.&lt;/p&gt;
&lt;p&gt;I had several minor problems with Tinkerer, many of which I worked around, but the main flaw was no timestamp support for blog posts.  All blog posts would have a date but not a time, so in the RSS feeds it would be as if they were posted at midnight.  If you are posting at 22:00 it means in feed aggregators your post would end up below many others posted that day and multiple posts in a day could be in any order.&lt;/p&gt;
&lt;p&gt;Today I bumped into a blogging platform called &lt;a class="reference external" href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;.  It too uses RST files to generate the site, but supports metadata in the RST files to signify things such as time of post.  It was incredibly easy to port my Tinkerer posts over so I gave it a try.&lt;/p&gt;
&lt;p&gt;I have ended up with generation scripts, RST files and a theme I have modified in a &lt;a class="reference external" href="https://github.com/LinuxJedi/linuxjedi.co.uk"&gt;GitHub repo&lt;/a&gt; and the generated content in my &lt;a class="reference external" href="https://github.com/LinuxJedi/linuxjedi.github.io"&gt;GitHub Pages Repo&lt;/a&gt;.  Pelican has a built-in HTTP server which makes it easy to preview your generated HTML before it is uploaded to the site.&lt;/p&gt;
&lt;p&gt;In conclusion, Tinkerer is a great platform, but Pelican feels more mature and it seems to have a wider community around it.  I also found its templates much easier to edit.  Both platforms have an Open Source feel to the way you create and publish content which is fantastic for my usage.  I think I have finally found a blogging platform I can settle with.&lt;/p&gt;
</summary><category term="Blog"></category><category term="LinuxJedi"></category></entry><entry><title>New Blog!</title><link href="http://linuxjedi.co.uk/posts/2014/Sep/23/new-blog/" rel="alternate"></link><updated>2014-09-23T16:45:00+01:00</updated><author><name></name></author><id>tag:linuxjedi.co.uk,2014-09-23:posts/2014/Sep/23/new-blog/</id><summary type="html">&lt;p&gt;Blogger is a great blogging platform.  Unfortunately it is really difficut to create content that has marked-up code in it.  Which as a developer is a requirement.  Therefore LinuxJedi's /dev/null has now moved to this GitHub pages site using &lt;a class="reference external" href="http://tinkerer.me/"&gt;Tinkerer&lt;/a&gt; to build it.&lt;/p&gt;
&lt;p&gt;The old site and content can still be accessed at &lt;a class="reference external" href="http://thelinuxjedi.blogspot.com/"&gt;http://thelinuxjedi.blogspot.com/&lt;/a&gt;.&lt;/p&gt;
</summary><category term="LinuxJedi"></category></entry></feed>