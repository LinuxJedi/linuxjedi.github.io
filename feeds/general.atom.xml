<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>LinuxJedi's /dev/null</title><link href="http://linuxjedi.co.uk/" rel="alternate"></link><link href="http://linuxjedi.co.uk/feeds/general.atom.xml" rel="self"></link><id>http://linuxjedi.co.uk/</id><updated>2015-04-23T22:27:00+01:00</updated><entry><title>Why Dependency Testing is Important</title><link href="http://linuxjedi.co.uk/posts/2015/Apr/23/why-dependency-testing-is-important/" rel="alternate"></link><published>2015-04-23T22:27:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-04-23:posts/2015/Apr/23/why-dependency-testing-is-important/</id><summary type="html">&lt;p&gt;Or...&lt;/p&gt;
&lt;div class="section" id="why-i-m-ditching-parallels"&gt;
&lt;h2&gt;Why I'm Ditching Parallels&lt;/h2&gt;
&lt;p&gt;I've been using Parallels quite successfully for several months now and I must admit it is a great way to run all my various Linux distros that I work with on my Mac.&lt;/p&gt;
&lt;p&gt;One of my main VMs is the latest Fedora release (currently Fedora 21). Fedora can be quite bleeding edge and in recent releases they have had a policy of a rolling kernel release inside the fixed twice-annual releases.&lt;/p&gt;
&lt;p&gt;A few weeks ago my Fedora VM ran an update which included an update to a 3.19 Kernel release. After a reboot my Fedora VM didn't start Xorg and was stuck at a service startup messages.&lt;/p&gt;
&lt;p&gt;I wasn't getting much response from Parallels support so I dug in to debug what had happened. It turns out that the Parallels tools had attempted to rebuild and failed. This meant that the required kernel modules were missing and a full boot could not happen. My Parallels had also updated so I could not switch to an older kernel, because this required a module rebuild and there development packages for the older kernels were not available.&lt;/p&gt;
&lt;p&gt;I ended up creating a quick temporary patch to fix this and &lt;a class="reference external" href="https://forum.parallels.com/threads/temporary-fix-for-kernel-3-19.328277/"&gt;posted it on Parallels forum&lt;/a&gt;. I eventually got a response from Parallels support stating they are working on the problem and to this day (now nearly 2 weeks after I created the patch) the problem has still not officially fixed by Parallels.&lt;/p&gt;
&lt;p&gt;Now, I understand that Parallels needs time to fix these things. But, this problem could have easily been resolved months ago, before it was even a problem to the users that have needed my patch.&lt;/p&gt;
&lt;p&gt;It comes down to a type of continuous integration that is relatively easy to set up. That is to test with new versions of your dependencies when they are upgraded.&lt;/p&gt;
&lt;p&gt;In the case of Linux this could be done by having a script that updates Arch Linux or similar daily, tries to install Parallels Tools and reports on failure. If Parallels had used this process with Arch Linux the problem would have been automatically detected in early February and they would have had time to fix it before it affected the bigger distros.&lt;/p&gt;
&lt;p&gt;This would cause a few false failures but those would be easy to weed out. They could even take a Fedora release and build a new kernel and other dependencies for it daily which will probably reduce the false failures.&lt;/p&gt;
&lt;p&gt;Today we are at a point where Ubuntu 15.04 has been released. The kernel with that distro is not compatible and there is no update to the proprietary Xorg driver that comes with Parallels for Xorg 1.17 that comes with this release. Making it completely unusable with Parallels.&lt;/p&gt;
&lt;p&gt;I need to use virtual machines for my day job at Nginx as I did in HP, so this problem is a real productivity killer and has meant I've had to finally give up and ditch Parallels. Today I've been trying VMWare Fusion instead and so far it is working great with every distro I need to use, even with updated kernel and Xorg.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="Testing"></category><category term="Parallels"></category></entry><entry><title>Live Kernel Patching - Why You Should NOT Use It</title><link href="http://linuxjedi.co.uk/posts/2015/Feb/14/live-kernel-patching-why-you-should-not-use-it/" rel="alternate"></link><published>2015-02-14T23:11:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-02-14:posts/2015/Feb/14/live-kernel-patching-why-you-should-not-use-it/</id><summary type="html">&lt;p&gt;Just under a year ago on my old blog I &lt;a class="reference external" href="http://thelinuxjedi.blogspot.co.uk/2014/03/live-kernel-patching.html"&gt;discussed&lt;/a&gt; and even &lt;a class="reference external" href="http://thelinuxjedi.blogspot.co.uk/2014/06/live-kernel-patching-video-demo.html"&gt;demoed&lt;/a&gt; the new Linux live kernel patching solutions.  I was reviewing these technologies out of my own curiosity as well as HP's Advanced Technology Group having an interest.  I think these technologies are great, I am personally more of a fan of the user experience of RedHat's kpatch solution but any solution is a great technical achievement.&lt;/p&gt;
&lt;p&gt;Having said this I believe that the use case for this technology is quite narrow.  Last time I looked into these technologies only patches that affected the code of functions could be modified.  Changing structs and data definitely didn't work and I suspect that changing function declarations was also dangerous.  There is also a performance overhead.  If you are replacing functions that are called many times a second the additional overhead of a jump to the replaced function will cause a performance hit.&lt;/p&gt;
&lt;p&gt;Where I can see live patching to be useful is on desktop machines that need critical patching but rebooting will cause losses in productivity or in science/academia where there are often long-running applications that cannot be paused for a reboot.&lt;/p&gt;
&lt;p&gt;The only place where I do not think it should be used and unfortunately I fear it will be the main place it is used is for Internet and network services.  Everyone should strive for 100% uptime, this is something I do not dispute.  But the 100% uptime should be for the &lt;em&gt;application&lt;/em&gt;, not necessarily the underlying hardware.&lt;/p&gt;
&lt;p&gt;100% uptime of hardware is not a possible reality.  Moving parts such as fans and hard disks fail, components degrade over time, power can fail along with redundancies and at some point you may need to upgrade or move hardware.  The solution to this is the same as the solution to kernel upgrades: multiple active servers.&lt;/p&gt;
&lt;p&gt;For web servers, have multiple servers load balanced, you can have multiple load balancers too.  Even multiple active MySQL servers are possible thanks to technologies such as &lt;a class="reference external" href="http://galeracluster.com/"&gt;Galera Cluster&lt;/a&gt;.  When the kernel needs updating you can apply a rolling reboot.  Technologies such as &lt;a class="reference external" href="http://www.ansible.com/"&gt;Ansible&lt;/a&gt; will even help you do this by making sure only a certain amount of servers from each tier of your architecture is updated at any time.&lt;/p&gt;
&lt;p&gt;I commend SUSE and RedHat in helping to replace a technology that otherwise was long since dead in the Open Source world.  Oracle seemingly killing the development of the Open Source KSplice was an annoyance to many.  But in today's world, is a technology we really need?&lt;/p&gt;
&lt;p&gt;I would be interested to hear in the comments if there is a use case I have missed.&lt;/p&gt;
&lt;div class="section" id="update"&gt;
&lt;h2&gt;Update&lt;/h2&gt;
&lt;p&gt;2015-02-14 23:11&lt;/p&gt;
&lt;p&gt;Robert Collins pointed out on Facebook that the overhead for the jump is likely to be too minimal to cause a performance issue and anything that is called too often is not worth patching (I am inclined to agree).&lt;/p&gt;
&lt;p&gt;Robert also points out that there is a cost involved in migrating many VMs when the hosts need upgrading for a zero-day.  I sort-of agree with this scenario.  For public cloud this is a problem, although I believe typically the rolling upgrade is performed by pausing VMs and resuming rather than migration.  With public cloud I feel this is acceptable with a short notice period.&lt;/p&gt;
&lt;p&gt;For private cloud there are ways around this issue, typically VMs are throw-away in private cloud so it is easy to just blow them away and recreate them in an already updated part of the data centre rather than migrate.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="Kernel"></category></entry><entry><title>Why The C10K Problem is Important</title><link href="http://linuxjedi.co.uk/posts/2015/Feb/14/why-the-c10k-problem-is-important/" rel="alternate"></link><published>2015-02-14T17:49:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-02-14:posts/2015/Feb/14/why-the-c10k-problem-is-important/</id><summary type="html">&lt;p&gt;For those who are not familiar with the concept the &lt;a class="reference external" href="http://www.kegel.com/c10k.html"&gt;C10K problem&lt;/a&gt; is about trying to handle many simultaneous clients on a web server.  Originally written about trying to handle 10,000 clients on a Gigabit Ethernet connection back in the late 90s, but this is still a problem today (with more clients and bigger connections).&lt;/p&gt;
&lt;p&gt;Websites are dealing with more and more traffic as not only more people every year are using the internet with more devices but they are also increasing their usage.  By this I mean not only in the amount they use the internet but higher resolution monitors and faster CPUs for interactive content means the amount of data for websites has increased too.  It has got to the point where dozens of files are needed to load a site such as Facebook or Twitter.&lt;/p&gt;
&lt;p&gt;Faster and faster internet connections have also meant that users are far more impatient.  I first used the Internet at home in 1995 with a 33.6Kbit/sec modem.  Websites would typically take at least 10 seconds to access and load, sometimes more.  This didn't bother us back then.  A decade later Internet speeds are much faster and sites are much bigger (javascript, css, high-res/retina images, flash, etc...), but users expect load times of only a couple of seconds and access/wait times of less than a second, even on cell phones.&lt;/p&gt;
&lt;p&gt;Although the server hardware is evolving it is difficult to keep up with client demand and thus the C10K problem is still an issue today.  One way of solving this is to have many servers or cloud instances handling the traffic.  This can cause a cost issue for many companies, especially startups.  The other solution which is gaining in popularity is event driven I/O.&lt;/p&gt;
&lt;p&gt;Typically with event driven I/O you run the server single threaded, although you can offload onto multiple threads/processes.  A single event loop is run which listens for new events such as data in or out and reacts based on the event.  In Linux this relies heavily in &lt;tt class="docutils literal"&gt;epoll()&lt;/tt&gt; and BSD based distributions &lt;tt class="docutils literal"&gt;kqueue()&lt;/tt&gt;.  These are edge-triggered &lt;tt class="docutils literal"&gt;poll()&lt;/tt&gt; replacements which notifies the application of a transition from a socket not-ready to ready.&lt;/p&gt;
&lt;p&gt;It does not feel natural to use a single-threaded solution in a world of multi-core CPUs but there is a wealth of evidence that this approach works very well.  It is in-general simpler to code since many of the complications of dealing with many threads such as race conditions and locking go away.  But there is the caveat that handling an event incorrectly could cause an application to hang.  The event-driven approach also typically reduces CPU usage of the application which on modern hardware can help with energy and heat savings.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://nginx.com/"&gt;Nginx&lt;/a&gt; is an example of a web/proxy server which uses event-driven design such as this.  Part of the reason Nginx is becoming so popular is that the design of it gives it the ability to scale much easier than more traditional web servers such as Apache.&lt;/p&gt;
&lt;p&gt;With &lt;a class="reference external" href="http://libattachsql.org/"&gt;libAttachSQL&lt;/a&gt; we at the Advanced Technology Group are doing something very similar, but this time on the client side.  So that client applications can handle many connections to one or more servers on a single thread with ease.  We already have plans to add even more features around this for the 2.0 release.&lt;/p&gt;
&lt;p&gt;Internally libAttachSQL uses &lt;a class="reference external" href="https://nikhilm.github.io/uvbook/introduction.html"&gt;libuv&lt;/a&gt; which is a high performance event driven I/O library born out of Node.js.  It also uses the event driven features of OpenSSL for SSL connections.&lt;/p&gt;
&lt;p&gt;I have heard many times that C10K is not an issue today due to the high performance hardware at much lower prices than were previously available.  It may not be a 10K problem any more, but it is still an issue.  Even the big boys of the Internet today are using event driven solutions to help them scale and keep the running costs low.  This is evident by the increased usage of technologies such as Nginx.&lt;/p&gt;
&lt;p&gt;In the last couple of years there is talk of the &lt;a class="reference external" href="http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html"&gt;C10M problem&lt;/a&gt; which suggests moving control of the TCP/IP stack completely away from the kernel to the user layer.  Personally I feel this is going a little too far for generic applications.  It could mean that your applications require specific OS/kernel and even specific hardware to run.  For people writing web servers and libraries this can easily bloat the code and by the time you have dealt with edge cases I suspect the performance increase will not be worth the extra work.&lt;/p&gt;
</summary><category term="HP"></category><category term="Advanced Technology Group"></category><category term="Nginx"></category><category term="libAttachSQL"></category></entry><entry><title>USB Flash Drives - Trimming the FAT</title><link href="http://linuxjedi.co.uk/posts/2015/Jan/05/usb-flash-drives-trimming-the-fat/" rel="alternate"></link><published>2015-01-05T22:34:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-01-05:posts/2015/Jan/05/usb-flash-drives-trimming-the-fat/</id><summary type="html">&lt;p&gt;As with most of you who read this blog I carry USB flash drives around with me all the time.  Right now I have 3 Kingston DTSE9 sticks on my keyring of various sizes each with a different purpose.  Whilst these drives are nowhere near the fastest out there they are the only ones I have had so far that don't snap off keyrings.&lt;/p&gt;
&lt;img alt="" src="/images/king16GB_DTSE9_2.jpg" /&gt;
&lt;p&gt;For this blog post I'll be talking about data where security is not a priority.  My encrypted flash drives are currently using &lt;a class="reference external" href="https://veracrypt.codeplex.com/"&gt;VeraCrypt&lt;/a&gt; but that is beyond the scope of this blog post.&lt;/p&gt;
&lt;div class="section" id="fat32-woes"&gt;
&lt;h2&gt;FAT32 woes&lt;/h2&gt;
&lt;p&gt;The largest one I have is 64GB and due to some of the work I do for HP's Advanced Technology Group this often needs to have large files on it.  Traditionally FAT32 has been used as a file system for memory cards and flash drives, one of the biggest reasons for this is that it is compatible with pretty much every computer operating system out there.  But FAT32 has many flaws:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;it has a single file limit of 4GB.  That may sound huge but this rules out DVD images and large databases.&lt;/li&gt;
&lt;li&gt;it does not support POSIX based permissions which anyone who uses Linux is used to.&lt;/li&gt;
&lt;li&gt;it doesn't support journalling so with a removable drive data corruption is very common.&lt;/li&gt;
&lt;li&gt;as a minor issue it is also designed for spinning disks with many more write cycles than flash.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What I needed was something that fixes as many of those problems as possible but is also compatible with Linux, Mac OS X and Windows.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="alternatives"&gt;
&lt;h2&gt;Alternatives&lt;/h2&gt;
&lt;p&gt;I looked into several alternatives to FAT32 and summarised my findings below.  I'm primarily looking for out-of-box support, I know there are paid third-party add-ons to operating systems to add support.&lt;/p&gt;
&lt;div class="section" id="exfat"&gt;
&lt;h3&gt;exFAT&lt;/h3&gt;
&lt;p&gt;A company looked into all the problems with FAT and created a filesystem designed for use with flash drives, they called this &lt;em&gt;exFAT&lt;/em&gt;.  Unfortunately that company is Microsoft and what they created is not an open standard and is full of patents.&lt;/p&gt;
&lt;p&gt;This means that only people who have paid licenses can use exFAT.  This includes Mac OS X and digital camera manufacturers but unfortunately means Linux support is very limited.&lt;/p&gt;
&lt;p&gt;exFAT is therefore thrown out.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="hfs"&gt;
&lt;h3&gt;HFS+&lt;/h3&gt;
&lt;p&gt;HFS+ is the primary file system used by Mac OS X.  There is good support for this in Macs (obviously) and Linux, but no support in Windows.&lt;/p&gt;
&lt;p&gt;Unfortunately that means HFS+ is out.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ntfs"&gt;
&lt;h3&gt;NTFS&lt;/h3&gt;
&lt;p&gt;NTFS is Microsoft's primary file system in NT based operating systems (for most people this means Windows XP onwards).  Like HFS+ it isn't a bad file system for flash drives and in recent Linux distributions has very good support.  Unfortunately in Mac OS X it can only be used in read-only mode.&lt;/p&gt;
&lt;p&gt;Another Microsoft FS thrown out.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="udf"&gt;
&lt;h3&gt;UDF&lt;/h3&gt;
&lt;p&gt;Yes, you read that correctly, UDF.  It is a filesystem which was originally designed for use with optical media.  But, it has since been adapted for use with hard drives and flash drives!&lt;/p&gt;
&lt;p&gt;The maximum file size is 16EB (actually bigger than the maximum volume size).  It supports POSIX file system permissions.  But most importantly, it works with Linux, Mac OS X and Windows (Vista onwards) out of the box!&lt;/p&gt;
&lt;p&gt;In addition the UDF format was designed for packet writing so it works by appending on the end of data on the file system and expiring the old data.  In theory this could lead to less wear of the drive.  Flash drives typically use dynamic wear leveling which is similar to the static wear leveling used in SSDs but less complex.  The algorithm used may mean that the packet writing has no real advantage on the wear of the drive.  I don't have enough data to say for certain.&lt;/p&gt;
&lt;p&gt;The file system itself works like a journal.  It appends new data to the end of the log with a new version of the file table.  So, if a write was not completed successfully it will use the previous version of the log.  This also means recovery of deleted files is possible by traversing previous versions of the data log.&lt;/p&gt;
&lt;p&gt;For me this ticks all the boxes so I am using it with a 64GB UDF formatted flash.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="making-a-udf-flash-drive"&gt;
&lt;h2&gt;Making a UDF flash drive&lt;/h2&gt;
&lt;div class="section" id="mac-os-x"&gt;
&lt;h3&gt;Mac OS X&lt;/h3&gt;
&lt;p&gt;Unfortunately Disk Utility doesn't let you format a flash drive as UDF but you can use the command line to do it.&lt;/p&gt;
&lt;p&gt;First of all you need to figure out the drive path for your flash drive.  It will be in the format &lt;tt class="docutils literal"&gt;/dev/disk{drive_no}&lt;/tt&gt; where drive_no is the drive number, if it is followed by the letter &lt;em&gt;s&lt;/em&gt; and another number then that is a partition and not what we need at this stage:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil list
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you need to find out the block size (it is typically 512).  Make a note of this number because you will need it later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil info /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;Block Size&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The drive can't be changed until we unmount the partitions so run this for every partition that is currently in-use for your drive:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil unmount /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt;s&lt;span class="o"&gt;{&lt;/span&gt;partition_no&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Due to the nature of the UDF format it is possible that the operating system would still detect the drive as FAT32 afterwards so we need to blank the drive with zeros.  This could take some time:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil secureErase &lt;span class="m"&gt;0&lt;/span&gt; /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now the drive can be formatted, replace &lt;em&gt;block_size&lt;/em&gt; with the number you wrote down above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo newfs_udf -b &lt;span class="o"&gt;{&lt;/span&gt;block_size&lt;span class="o"&gt;}&lt;/span&gt; /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally the drive can be mounted again for use as normal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ diskutil mount /dev/disk&lt;span class="o"&gt;{&lt;/span&gt;drive_no&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="linux"&gt;
&lt;h3&gt;Linux&lt;/h3&gt;
&lt;p&gt;In Linux things get a little easier.  First of all unmount the partitions on the drive and then we need the block size, write this one down:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo blockdev --getbsz /dev/sd&lt;span class="o"&gt;{&lt;/span&gt;drive_letter&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We then need to zero out the drive so that it isn't incorrectly detected:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo dd &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/dev/zero &lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/dev/sd&lt;span class="o"&gt;{&lt;/span&gt;drive_letter&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="nv"&gt;bs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1M &lt;span class="nv"&gt;count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we need to make the UDF format, replacing &lt;em&gt;block_size&lt;/em&gt; with the number noted above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo mkudffs -b &lt;span class="o"&gt;{&lt;/span&gt;block_size&lt;span class="o"&gt;}&lt;/span&gt; --media-type&lt;span class="o"&gt;=&lt;/span&gt;hd /dev/sd&lt;span class="o"&gt;{&lt;/span&gt;drive_letter&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="USB"></category><category term="Flash"></category><category term="Filesystems"></category><category term="HP"></category><category term="Advanced Technology Group"></category></entry><entry><title>Family Travel Technology</title><link href="http://linuxjedi.co.uk/posts/2014/Dec/28/family-travel-technology/" rel="alternate"></link><published>2014-12-28T16:18:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-12-28:posts/2014/Dec/28/family-travel-technology/</id><summary type="html">&lt;p&gt;My family and I spent a few days before Christmas visiting family away from home.  We booked two interconnecting hotel rooms so our children could have their own room and we could sleep with a little bit of peace.  As with many trips there is often a period between dinner and sleep where you are stuck in a hotel room with nothing to do so this time I devised a plan for entertainment.&lt;/p&gt;
&lt;p&gt;First of all, we all have internet connected devices, between phones, gaming devices, my laptop, etc... we actually had 9 devices with us (some I will talk about later in this post).  As with many hotel WiFi plans there is a cap on the number of devices you can have assigned to your room.  The WiFi itself was quite speedy for a hotel and was fast enough to share across several devices.  To get around this limit and also to create our own personal network I used a &lt;a class="reference external" href="http://www.netgear.co.uk/home/products/networking/wifi-routers/PR2000.aspx"&gt;Netgear PR2000 Travel Router&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="" src="/images/netgear.jpg" /&gt;
&lt;p&gt;The Netgear router connects to a public network via. a network cable or WiFi and creates its own WiFi/wired network for you to use.  This means you use any one of your devices to log into the hotel WiFi and to the hotel it looks like just the router is connected.  This is a fantastic device and I benchmarked it before I went on vacation, I was easily pushing 20mbit over the internet in both directions with it in less than ideal conditions so it is was perfect for travelling with.  I absolutely love this device and I think the only flaw I have found with it is that it gets into a reboot loop after you first configure it and have to power cycle it to make it work.&lt;/p&gt;
&lt;p&gt;The only thing I would like to see improved with this device is the inclusion of a US power connector.  It can be powered via. USB or by clipping on an included EU or UK power plug.  If you power it using a wall plug connection you can use the USB port on the device to share a USB stick or drive across your private network.  That said I've tested and a C7 (figure of 8) power cable will be compatible if not elegant to look at, much like using a C7 in an Apple Mac power adaptor.&lt;/p&gt;
&lt;p&gt;As part of the entertainment we wanted a movie night.  The children had a selection of films they wanted to watch and I had already transcoded them from their DVDs onto my laptop.  My wife and I are currently watching box sets of the TV series of 24 and we wanted to watch this.  So, in the children's room I hooked my Apple TV into their TV and in our room I used the HDMI on my Macbook Pro (2014 13&amp;quot; retina model) in our room.  The Macbook Pro does not have an included DVD drive, luckily earlier this year I bought a USB &lt;a class="reference external" href="https://www.samsung.com/uk/consumer/memory-cards-hdd-odd/odd/odd/SE-S084D/TSBS"&gt;Samsung S084D DVD drive&lt;/a&gt; second hand for Â£4.&lt;/p&gt;
&lt;p&gt;To stream the video to the Apple TV in the children's room I used an application called &lt;a class="reference external" href="http://beamer-app.com/"&gt;Beamer&lt;/a&gt;.  This converted the video on-the-fly to a format the Apple TV could use and streamed it.  It has worked great with every file format I have thrown at it and automatically find the Apple TV with no problems.  With the time slider on it I could also see how long was left on the film before they had finished with it.  Whilst in the parents room we were using Apple's built-in DVD player.&lt;/p&gt;
&lt;p&gt;The whole setup worked perfectly, throughout the stay the children watched two films on the Apple TV using Beamer and we got half way through a series of 24.  Being my geeky self I'm impressed at how well the setup glued together mostly with parts I already had at home.  The whole lot fitted easily in my backpack and I had lots of space to spare for other things.&lt;/p&gt;
</summary><category term="Apple"></category><category term="Netgear"></category><category term="Beamer"></category></entry><entry><title>Blogging Platforms</title><link href="http://linuxjedi.co.uk/posts/2014/Oct/04/blogging-platforms/" rel="alternate"></link><published>2014-10-04T22:16:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-10-04:posts/2014/Oct/04/blogging-platforms/</id><summary type="html">&lt;p&gt;A couple of weeks ago I ditched Blogger as my main blogging platform.  The main reason for this was the editing tools were breaking posts containing code.  Whilst it is a great platform for basic blogging, it is not suitable for developers blogs.&lt;/p&gt;
&lt;p&gt;So, I was on the hunt for blogging platforms that would make it easy for me to write posts that contain technical content and is not expensive to run.  I also don't want to be maintaining my own web server, I may be capable of doing this but I don't want the time overhead.&lt;/p&gt;
&lt;p&gt;I tried several things out that met some of my requirements but many didn't fit all.  Wordpress was probably the closest, but I had trouble bending the free templates to my will.&lt;/p&gt;
&lt;p&gt;With many on my team at HP's Advanced Technology Group using &lt;a class="reference external" href="https://pages.github.com/"&gt;GitHub Pages&lt;/a&gt; for blog posts I thought I would give it a try.  Most of the team are trying out Jekyll which looks really good, but isn't for me.  I prefer &lt;a class="reference external" href="http://docutils.sourceforge.net/rst.html"&gt;reStructuredText&lt;/a&gt; to Markdown and use it every day for the &lt;a class="reference external" href="http://docs.libattachsql.org"&gt;libAttachSQL documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On my journey I came across &lt;a class="reference external" href="http://tinkerer.me/"&gt;Tinkerer&lt;/a&gt; which is a layer on top of &lt;a class="reference external" href="http://sphinx-doc.org/"&gt;Python Sphinx&lt;/a&gt; to generate a blog site from RST files.  This was great for me because Sphinx is the renderer used for libAttachSQL's docs both in the build system and &lt;a class="reference external" href="https://readthedocs.org/"&gt;Read The Docs&lt;/a&gt;.  I created a new blog on this hosted on GitHub Pages and &lt;a class="reference external" href="https://disqus.com/"&gt;Disqus&lt;/a&gt; for comments.&lt;/p&gt;
&lt;p&gt;I had several minor problems with Tinkerer, many of which I worked around, but the main flaw was no timestamp support for blog posts.  All blog posts would have a date but not a time, so in the RSS feeds it would be as if they were posted at midnight.  If you are posting at 22:00 it means in feed aggregators your post would end up below many others posted that day and multiple posts in a day could be in any order.&lt;/p&gt;
&lt;p&gt;Today I bumped into a blogging platform called &lt;a class="reference external" href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;.  It too uses RST files to generate the site, but supports metadata in the RST files to signify things such as time of post.  It was incredibly easy to port my Tinkerer posts over so I gave it a try.&lt;/p&gt;
&lt;p&gt;I have ended up with generation scripts, RST files and a theme I have modified in a &lt;a class="reference external" href="https://github.com/LinuxJedi/linuxjedi.co.uk"&gt;GitHub repo&lt;/a&gt; and the generated content in my &lt;a class="reference external" href="https://github.com/LinuxJedi/linuxjedi.github.io"&gt;GitHub Pages Repo&lt;/a&gt;.  Pelican has a built-in HTTP server which makes it easy to preview your generated HTML before it is uploaded to the site.&lt;/p&gt;
&lt;p&gt;In conclusion, Tinkerer is a great platform, but Pelican feels more mature and it seems to have a wider community around it.  I also found its templates much easier to edit.  Both platforms have an Open Source feel to the way you create and publish content which is fantastic for my usage.  I think I have finally found a blogging platform I can settle with.&lt;/p&gt;
</summary><category term="Blog"></category><category term="LinuxJedi"></category></entry><entry><title>New Blog!</title><link href="http://linuxjedi.co.uk/posts/2014/Sep/23/new-blog/" rel="alternate"></link><published>2014-09-23T16:45:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2014-09-23:posts/2014/Sep/23/new-blog/</id><summary type="html">&lt;p&gt;Blogger is a great blogging platform.  Unfortunately it is really difficut to create content that has marked-up code in it.  Which as a developer is a requirement.  Therefore LinuxJedi's /dev/null has now moved to this GitHub pages site using &lt;a class="reference external" href="http://tinkerer.me/"&gt;Tinkerer&lt;/a&gt; to build it.&lt;/p&gt;
&lt;p&gt;The old site and content can still be accessed at &lt;a class="reference external" href="http://thelinuxjedi.blogspot.com/"&gt;http://thelinuxjedi.blogspot.com/&lt;/a&gt;.&lt;/p&gt;
</summary><category term="LinuxJedi"></category></entry></feed>