<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>LinuxJedi's /dev/null</title><link href="http://linuxjedi.co.uk/" rel="alternate"></link><link href="http://linuxjedi.co.uk/feeds/nginx.atom.xml" rel="self"></link><id>http://linuxjedi.co.uk/</id><updated>2016-01-02T14:10:00+00:00</updated><entry><title>Multi-Gigabit Networking on a Budget</title><link href="http://linuxjedi.co.uk/posts/2016/Jan/02/multi-gigabit-networking-on-a-budget/" rel="alternate"></link><published>2016-01-02T14:10:00+00:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2016-01-02:posts/2016/Jan/02/multi-gigabit-networking-on-a-budget/</id><summary type="html">&lt;p&gt;I've recently acquired two &amp;quot;refurbished&amp;quot; Xeon workstations from eBay to do some &lt;a class="reference external" href="https://www.nginx.com/"&gt;NGINX&lt;/a&gt; testing with. These are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A Lenovo ThinkStation S30 - 6 core / 12 thread E5-1650v2 &amp;#64;3.5GHz, 4GB ECC RAM (upgraded to 12GB ECC - £10) - £650&lt;/li&gt;
&lt;li&gt;A Lenovo ThinkStation C20 - 2x 4 core / 8 thread (8 core / 16 thread total) E5620 &amp;#64;2.4GHz, 12GB ECC RAM - £200&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can maybe tell, I'm a fan of Lenovo hardware, I use a Thinkpad x220 still as my primary laptop despite also having newer laptops.&lt;/p&gt;
&lt;img alt="Lenovo ThinkStation S30 and C20" src="/images/xeon_workstations.jpg" style="width: 400px;" /&gt;
&lt;p&gt;The S30 was actually brand new, still sealed in box with Lenovo tape. Both machines were in great condition and run NGINX very well... Too well... The gigabit LAN in these was easily saturated without any real effort from the rest of the machines.&lt;/p&gt;
&lt;p&gt;I decided to look into ways of upgrading the network on these machines but without spending a fortune doing it.&lt;/p&gt;
&lt;div class="section" id="quad-port-bonded-gigabit"&gt;
&lt;h2&gt;Quad-port Bonded Gigabit&lt;/h2&gt;
&lt;p&gt;First up I looked into 4-port Gigabit network cards. For £40 each I got hold of an Intel Pro 1000 ET and an Intel Pro 1000 PT. The main difference between these cards appears to be the number of interrupt channels per port. The ET has 4 per port, the PT has 1. I have an HP ProCurve 16-port Gigabit switch I luckily picked up on eBay for £10 a while back so both cards were hooked into this using lots of short runs of CAT5e cabling.&lt;/p&gt;
&lt;p&gt;Intel Pro 1000 ET:&lt;/p&gt;
&lt;img alt="Intel Pro 1000 ET" src="/images/1000et.jpg" style="width: 400px;" /&gt;
&lt;p&gt;Intel Pro 1000 PT:&lt;/p&gt;
&lt;img alt="Intel Pro 1000 PT" src="/images/1000pt.jpg" style="width: 400px;" /&gt;
&lt;p&gt;I was using Fedora 23 in both machines, RedHat have an excellent guide on how to bond network connections using NetworkManager's CLI tool which can be seen &lt;a class="reference external" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/sec-Network_Bonding_Using_the_NetworkManager_Command_Line_Tool_nmcli.html"&gt;here&lt;/a&gt;. I also installed irqbalance so that I didn't have to worry too much about pinning the network card interrupts to specific CPUs (by default they are all pinned to the first CPU). This generally isn't the recommended way of doing it, &lt;a class="reference external" href="http://linuxjedi.co.uk/posts/2015/Jul/06/hitting-network-limits/"&gt;setting RPS&lt;/a&gt; is considered better, but worked fine for my testing.&lt;/p&gt;
&lt;p&gt;I used &lt;a class="reference external" href="http://software.es.net/iperf/"&gt;iperf3&lt;/a&gt; for measuring the bandwidth between the cards and did no tuning apart from setting up irqbalance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ iperf3 -Z -N -P4 -c ngx4
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The result summary showed almost exactly 3GBit/sec being transferred between the cards. This tallys with what was being observed using NGINX which was peaking at around 3.5GBit/sec with a little bit of tuning. The biggest problem with these cards and NGINX was the amount of connections-per-second. Whilst the Pro 1000 ET could easily handle the number of interrupts required for a high number of Packets Per Second, the Pro 1000 PT got very saturated quickly. Having a high number of PPS is vital to handling many connections per second with small amounts of data being transferred.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="gigabit-ethernet"&gt;
&lt;h2&gt;10 Gigabit Ethernet&lt;/h2&gt;
&lt;p&gt;I spent a while looking at alternative solutions for even faster networks. I looked into Inifiband, Dolphin Interconnect and other similar technologies. All designed for fast low-latency connections. I almost bought an Inifiband based solution when I stumbled across a pair of Mellanox ConnectX 2 10GbE adaptors. Ordering the pair of these came to £24.81 (plus £10.71 shipping from the US). They use SFP+ connectors so I bought a copper 1 meter SFP+ cable for £4.&lt;/p&gt;
&lt;p&gt;Mellanox ConnectX 2:&lt;/p&gt;
&lt;img alt="Mellanox ConnectX 2" src="/images/connectx.jpg" style="width: 400px;" /&gt;
&lt;p&gt;I plugged them in to each and set them up with static IPs and they worked great out-the-box. The Linux driver is showing 8 interrupts for the cards and irqbalance handles these nicely. With iperf executed as before the results are 9.4GBit/sec transferred through these cards.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;I have learned a lot from this, especially since this is my first physical exposure to 10GBit networking (I've configured software for them, just never wired them). The 4-port Intel NICs are amazing and I would highly recommend the Intel Pro 1000 ET or VT if you are in the market for a used card, the PT cards are good too but can't handle as many PPS.&lt;/p&gt;
&lt;p&gt;I'm extremely impressed by the performance of the 10GbE cards for such a low price and they would make great cards to do large file transfers (video work?) between machines in a home. I haven't yet found an affordable switch yet that has more than 2 SFP+ ports but I'm keeping my eye out for one. I'm also on the lookout for a cheap way of doing 40GbE connections but it may be a little too soon to do this kind of testing.&lt;/p&gt;
&lt;p&gt;I love the Lenovo ThinkStations and would have been equally happy with HP's Z-series, but couldn't find them at the right price-point. The Ivy-Bridge based S30 would actually make an amazing gaming machine if paired with a good graphics card. For me, they are both fantastic dev/test machines (and very fast at compiling). I've already used them to debug a lot of code and I'm looking forward to testing more hardware and software with them in the future.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="Nginx"></category><category term="networking"></category></entry><entry><title>Hitting Network Limits</title><link href="http://linuxjedi.co.uk/posts/2015/Jul/06/hitting-network-limits/" rel="alternate"></link><published>2015-07-06T18:10:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-07-06:posts/2015/Jul/06/hitting-network-limits/</id><summary type="html">&lt;p&gt;I recently wrote a high-level blog post for NGINX &lt;a class="reference external" href="https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/"&gt;outlining the new SO_REUSEPORT feature in NGINX&lt;/a&gt;. One of the problems I was having whilst doing the benchmarks for this was that I was hitting some kind of bottleneck before hitting the limit of what NGINX could process. This meant that it was difficult to get useful benchmarks.&lt;/p&gt;
&lt;p&gt;The main reason was down to hitting a limit on the maximum number of interrupts per second that could be processed. Despite the evidence staring me in the face it didn't click with me that all the interrupts were being processed on just one CPU.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PID   USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
3     root      20   0       0      0      0 R  95.4  0.0   0:10.28 ksoftirqd/0
21076 nobody    20   0   57208  31140   2272 S  26.5  1.0   9:41.90 nginx
21075 nobody    20   0   57164  31200   2376 S  26.2  1.0   9:47.15 nginx
21078 nobody    20   0   57236  31272   2376 S  25.8  1.0   9:40.71 nginx
21077 nobody    20   0   57184  31112   2268 R  25.2  1.0   9:40.94 nginx
&lt;/pre&gt;
&lt;p&gt;This is an example on my home test rig. The server is based on a Core2Quad CPU, nothing special but it helps pin bottlenecks a lot of the time. NGINX in this example has been configured with &lt;tt class="docutils literal"&gt;SO_REUSEPORT&lt;/tt&gt; enabled and we are returning just a very small packet response. The thing to notice here is that each NGINX worker is only using around 25-26% CPU (theoretically this machine has 400% CPU possible), and 95% CPU is being used for &lt;tt class="docutils literal"&gt;ksoftirqd&lt;/tt&gt;. This last part is the key, it basically means that CPU 0 is being used up nearly 100% dealing with network interrupts, hitting a limit of what the machine can process. Whilst I very quickly figured that network interrupts were the bottleneck the above didn't click with me at the time to be the issue.&lt;/p&gt;
&lt;p&gt;It turns out in Linux there is a way to spread these interrupts across all CPUs, I stumbled upon it by accident earlier today (and am kicking myself that I wasn't aware of it before). It is called &lt;strong&gt;Receive Packet Steering&lt;/strong&gt; (RPS). &lt;a class="reference external" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/network-rps.html"&gt;RedHat's Performance Tuning Guide&lt;/a&gt; has a good description of what it does and how to use it. To enable it on a quad-core machine simply do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;echo&lt;/span&gt; f &amp;gt; /sys/class/net/eth0/queues/rx-0/rps_cpus
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Obviously replacing &lt;em&gt;eth0&lt;/em&gt; with the name of your network card. Once enabled I was getting much better results:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PID   USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
21076 nobody    20   0   56140  29988   2272 R  93.8  1.0   9:12.14 nginx
21075 nobody    20   0   56152  30008   2376 R  92.9  1.0   9:17.54 nginx
21077 nobody    20   0   55996  29844   2268 R  92.4  1.0   9:11.67 nginx
21078 nobody    20   0   56160  30196   2376 R  91.7  1.0   9:11.85 nginx
&lt;/pre&gt;
&lt;p&gt;The client machine (which was also configured with this parameter) was reading 2.5x the traffic and nearly half the latency. In addition there is &lt;a class="reference external" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/network-rfs.html"&gt;Receive Flow Steering (RFS)&lt;/a&gt; which can reduce latency when RPS is used, in my testing the average latency was the same but the standard deviation was reduced.&lt;/p&gt;
&lt;p&gt;Network cards that can spread the hardware interrupts and stacks using DPDK or similar probably won't have this issue. But for cloud environments I suspect this option will help a great deal. As always &amp;quot;Your Mileage May Vary&amp;quot;.&lt;/p&gt;
&lt;p&gt;If you have any tips for increasing performance please feel free to leave them in the comments.&lt;/p&gt;
</summary><category term="Nginx"></category><category term="networking"></category></entry><entry><title>The LinuxJedi at NGINX</title><link href="http://linuxjedi.co.uk/posts/2015/Jun/26/the-linuxjedi-at-nginx/" rel="alternate"></link><published>2015-06-26T21:40:00+01:00</published><author><name></name></author><id>tag:linuxjedi.co.uk,2015-06-26:posts/2015/Jun/26/the-linuxjedi-at-nginx/</id><summary type="html">&lt;p&gt;Several months ago I was having a Sunday brunch with some friends in Seattle. One of the discussions that came up was the fact that &lt;a class="reference external" href="http://nginx.com/"&gt;NGINX&lt;/a&gt; needed to expand their developer relations team and we were thinking up names of likely candidates for the role. Fast forward to today and I've somehow wound up working at NGINX on the developer relations team!&lt;/p&gt;
&lt;p&gt;My role is &amp;quot;Senior Developer Advocate&amp;quot; and there are many parts of my role, but I guess it could be summarised as a cross between a community manager and community developer.&lt;/p&gt;
&lt;p&gt;I made many friends during my time at HP and it was hard leaving them, especially the Advanced Technology Group since this was the last team I worked in. Brian Aker has been my boss and mentor for many years through several companies and in particular he will always be a good friend.&lt;/p&gt;
&lt;div class="section" id="nginx-conference"&gt;
&lt;h2&gt;NGINX Conference&lt;/h2&gt;
&lt;p&gt;The day it became public that I was joining NGINX I got lots of people coming up to me telling me that NGINX had solved problems for them in the past. Even giving me thanks (which was a little strange because I hadn't even started yet).&lt;/p&gt;
&lt;p&gt;It seems everyone I meet has a unique and interesting story about their NGINX implemention. It is fantastic to work with a team that creates a tool that can be used in so many unique ways, many of which we could never imagine.&lt;/p&gt;
&lt;p&gt;This is where NGINX needs you, everyone has a unique story about NGINX and we want to give you the opportunity to tell yours. We have the &lt;a class="reference external" href="http://nginx.com/nginxconf/"&gt;NGINX Conference&lt;/a&gt; on September 22-24th 2015 and the CfP is still open for just under 1 week. So please come and submit your story!&lt;/p&gt;
&lt;p&gt;I will be at the conference and we have lots of fun and interesting things already planned.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nginx-so-far"&gt;
&lt;h2&gt;NGINX so far&lt;/h2&gt;
&lt;p&gt;The reason I haven't written a post in a while is NGINX has kept me so incredibly busy, but it has been a lot of fun too. That isn't to say I haven't been blogging, you can see a post I wrote called &lt;a class="reference external" href="http://nginx.com/blog/socket-sharding-nginx-release-1-9-1/"&gt;Socket Sharding in NGINX Release 1.9.1&lt;/a&gt; on the NGINX blog.&lt;/p&gt;
&lt;p&gt;I've flown to San Francisco and Moscow to meet the teams there. Everyone at NGINX is passionate about the product and it is a fantastic thing to see.&lt;/p&gt;
&lt;p&gt;I have also spoken to many members of the NGINX community as well as companies big and small who use NGINX as a part of their stack. All of them very enthusiastic about NGINX and excited to see what we have lined up for future releases (trust me, we have some amazing features lined up).&lt;/p&gt;
&lt;p&gt;There is a lot of work for me to do here but I'm enjoying every minute of it. I'm here to help the Open Source community and we have an awesome community, so lively that it is hard for me to keep up. I will try and take time out to blog more about things I'm working on at NGINX over the coming months.&lt;/p&gt;
&lt;p&gt;I look forward to the future of NGINX and seeing as many of you as I can at the &lt;a class="reference external" href="http://nginx.com/nginxconf/"&gt;NGINX Conference&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="Nginx"></category></entry></feed>